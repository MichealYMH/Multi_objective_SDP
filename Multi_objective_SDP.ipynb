{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "F:\\Programming\\Anaconda\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# 多输出模型\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "#原始的导入\n",
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,RepeatVector, Bidirectional\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, TimeDistributed, LSTM, SimpleRNN, GRU, Conv1D,Conv2D\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 安装graphviz的路径\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'F:/Tools/Graphviz/bin/'  # 安装graphviz的路径\n",
    "\n",
    "import time \n",
    "\n",
    "import psutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessData1(train_df,test_df):\n",
    "    df1=train_df.drop(['name'],axis = 1)\n",
    "#     df1=train_df\n",
    "    ndarray1 = df1.values\n",
    "    Label1 = ndarray1[:,:1]\n",
    "    Features1 = ndarray1[:,1:]\n",
    "    minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "    scaledFeatures1 = minmax_scale.fit_transform(Features1)\n",
    "    \n",
    "    df2=test_df.drop(['name'],axis = 1)\n",
    "#     df2=test_df\n",
    "    ndarray2 = df2.values\n",
    "    Label2 = ndarray2[:,:1]\n",
    "    Features2 = ndarray2[:,1:]\n",
    "    scaledFeatures2 = minmax_scale.transform(Features2)\n",
    "    \n",
    "    return scaledFeatures1,Label1,scaledFeatures2,Label2\n",
    "\n",
    "def PreprocessData2(train_df,test_df):\n",
    "    df1=train_df.drop(['name'],axis = 1)\n",
    "#     df1=train_df\n",
    "    ndarray1 = df1.values\n",
    "    Label1 = ndarray1[:,:8]\n",
    "    Features1 = ndarray1[:,8:]\n",
    "    minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "    scaledFeatures1 = minmax_scale.fit_transform(Features1)\n",
    "    \n",
    "    df2=test_df.drop(['name'],axis = 1)\n",
    "#     df2=test_df\n",
    "    ndarray2 = df2.values\n",
    "    Label2 = ndarray2[:,:8]\n",
    "    Features2 = ndarray2[:,8:]\n",
    "    scaledFeatures2 = minmax_scale.transform(Features2)\n",
    "    \n",
    "    return scaledFeatures1,Label1,scaledFeatures2,Label2\n",
    "\n",
    "def PreprocessData3(train_df,test_df):\n",
    "    df1=train_df.drop(['name'],axis = 1)\n",
    "#     df1=train_df\n",
    "    ndarray1 = df1.values\n",
    "    Label1 = ndarray1[:,:20]\n",
    "    Features1 = ndarray1[:,20:]\n",
    "    minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "    scaledFeatures1 = minmax_scale.fit_transform(Features1)\n",
    "    \n",
    "    df2=test_df.drop(['name'],axis = 1)\n",
    "#     df2=test_df\n",
    "    ndarray2 = df2.values\n",
    "    Label2 = ndarray2[:,:20]\n",
    "    Features2 = ndarray2[:,20:]\n",
    "    scaledFeatures2 = minmax_scale.transform(Features2)\n",
    "    \n",
    "    return scaledFeatures1,Label1,scaledFeatures2,Label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_history(train_history,train1,train2,train3,validation1,validation2,validation3):\n",
    "    plt.plot(train_history.history[train1],\",b-\")\n",
    "    plt.plot(train_history.history[train2],\",r-\")\n",
    "    plt.plot(train_history.history[train3],\",g-\")\n",
    "    plt.plot(train_history.history[validation1],\",b:\")\n",
    "    plt.plot(train_history.history[validation2],\",r:\")\n",
    "    plt.plot(train_history.history[validation3],\",g:\")\n",
    "    plt.title('Training History')\n",
    "    plt.ylabel('Acc/Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train3','validation3'])\n",
    "    plt.show()\n",
    "    \n",
    "def get_topK(label: list, prediction: list, k: int):\n",
    "    label = [int(i) for i in label]\n",
    "    prediction = [float(i) for i in prediction]\n",
    "    hit_the_label = 0\n",
    "    k_hit = 0\n",
    "    # 拿到预测值和0.5的距离\n",
    "    Lst = [abs(0.5 - i) for i in prediction]\n",
    "    n = 4\n",
    "    index_n = []\n",
    "    for i in range(n):\n",
    "        index_i = Lst.index(min(Lst))\n",
    "        index_n.append(index_i)\n",
    "        Lst[index_i] = float('inf')\n",
    "    # 全部取大于0.5的，概率最大\n",
    "    pred_label = [1 if i > 0.5 else 0 for i in prediction]\n",
    "    group = []\n",
    "\n",
    "    sub = subsets([i for i in range(n)])\n",
    "\n",
    "    def rev(rev_list):\n",
    "        tmp = pred_label[:]\n",
    "        for i in rev_list:\n",
    "            tmp[index_n[i]] = 1 - tmp[index_n[i]]\n",
    "        return tmp\n",
    "    for i in sub:\n",
    "        group.append(rev(i))\n",
    "\n",
    "    # 计算每种组合的出现概率，prediction代表取1的概率\n",
    "    group_prob = []\n",
    "    for pred in group:\n",
    "        p = 1\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] == 1:\n",
    "                p *= prediction[i]\n",
    "            else:\n",
    "                p *= (1 - prediction[i])\n",
    "        group_prob.append(p)\n",
    "\n",
    "    # 拿到概率最大的k个组合的索引\n",
    "    Lst = list(group_prob[:])\n",
    "    index_k = []\n",
    "    for i in range(k):\n",
    "        index_i = Lst.index(max(Lst))\n",
    "        index_k.append(index_i)\n",
    "        Lst[index_i] = float('-inf')\n",
    "\n",
    "    # 判断概率最大的k个组合 是否有某个组合命中label\n",
    "    for idx in range(len(index_k)):\n",
    "        if hit_the_label:\n",
    "            break\n",
    "        pred_label = group[index_k[idx]]\n",
    "        hit = True\n",
    "\n",
    "        for i in range(len(pred_label)):\n",
    "            if pred_label[i] != label[i]:\n",
    "                hit = False\n",
    "        if hit:\n",
    "            hit_the_label = 1\n",
    "            k_hit = idx + 1\n",
    "        # print(pred_label, label, hit_the_label, idx, index_k)\n",
    "    # print(group)\n",
    "    # for i in range(len(group_prob)):\n",
    "    #     print(i, group_prob[i])\n",
    "    return hit_the_label, k_hit\n",
    "    \n",
    "def subsets(num_list):\n",
    "    res = []\n",
    "    path = []\n",
    "\n",
    "    def backtrack(nums, start_index):\n",
    "        res.append(path[:])  # 收集子集，要放在终止添加的上面，否则会漏掉自己\n",
    "        for i in range(start_index, len(nums)):  # 当startIndex已经大于数组的长度了，就终止了，for循环本来也结束了，所以不需要终止条件\n",
    "            path.append(nums[i])\n",
    "            backtrack(nums, i + 1)  # 递归\n",
    "            path.pop()  # 回溯\n",
    "    backtrack(num_list, 0)\n",
    "    return res\n",
    "\n",
    "def trans(z):\n",
    "    return [round(n) for n in z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型评估方法\n",
    "\n",
    "def eval1(predProb2,test_Label):\n",
    "    print(\"……………………………………………………………………………………………\")\n",
    "    print(\"开始进行模型评估\")\n",
    "    \n",
    "    predLabel = [0*x for x in range(len(predProb2))]\n",
    "    for i in range(len(predProb2)):\n",
    "        predLabel[i] =  trans(predProb2.tolist()[i])\n",
    "    y_true = test_Label\n",
    "    print(type(y_true))\n",
    "    # print(y_true)\n",
    "    y_pred = predLabel\n",
    "    print(type(y_pred))\n",
    "    # print(y_pred)\n",
    "    \n",
    "    score1 = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    score2 = recall_score(y_true, y_pred, average='micro')\n",
    "\n",
    "    score3 = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    score4 = recall_score(y_true, y_pred, average=None)\n",
    "\n",
    "    score5 = precision_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    score6 = precision_score(y_true, y_pred, average='micro')\n",
    "\n",
    "    score7 = precision_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    score8 = precision_score(y_true, y_pred, average=None)\n",
    "\n",
    "    score9 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    score10 = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "    score11 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    score12 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    score13 = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    num_defect = 0\n",
    "    for i in range(len(test_Label)):\n",
    "        if test_Label[i] == 1:\n",
    "            num_defect += 1\n",
    "    print(num_defect/len(test_Label))\n",
    "    print(\"|||||||||||||||||||||||||||||||\")\n",
    "    print('缺陷率')\n",
    "    print('accuracy')\n",
    "    print('recall-avg')\n",
    "    print('recall-0-defect')\n",
    "    print('recall-1-nodefect')\n",
    "    print('precision-avg')\n",
    "    print('precision-0-defect')\n",
    "    print('precision-1-nodefect')\n",
    "    print('F1-score')\n",
    "    print('F1-score-0-defect')\n",
    "    print('F1-score-1-nodefect')\n",
    "    print(\"|||||||||||||||||||||||||||||||\")\n",
    "    print(num_defect/len(test_Label))  \n",
    "    print(score13)\n",
    "    print(score1)\n",
    "    print(score4[0])\n",
    "    print(score4[1])\n",
    "    print(score5)\n",
    "    print(score8[0])\n",
    "    print(score8[1])\n",
    "    print(score9)\n",
    "    print(score12[0])\n",
    "    print(score12[1])\n",
    "    print(\"|||||||||||||||||||||||||||||||\")\n",
    "    print(\"=====================================================================\")\n",
    "    \n",
    "\n",
    "def eval2(predProb2,test_df):\n",
    "    print(\"……………………………………………………………………………………………\")\n",
    "    print(\"……………………………………………………………………………………………\")\n",
    "    print(\"……………………………………………………………………………………………\")\n",
    "    print(\"开始进行任务2评估\")\n",
    "    print(\"###############\",software,\"###############\")\n",
    "    test_result = pd.DataFrame()\n",
    "    for i in range(len(test_df)):\n",
    "        test_result[i] =  predProb2[i]\n",
    "\n",
    "    label_result = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(test_df)):\n",
    "        label_result[i] =  trans(predProb2[i])\n",
    "\n",
    "    test_data = np.array(test_df).tolist()\n",
    "    sub_acc = 0\n",
    "    sum_acc = 0\n",
    "    \n",
    "    print(\"Top1的sub-accuracy\")\n",
    "    for i in range(len(test_df)):\n",
    "#         print(test_data[i][0:8])\n",
    "#         print(label_result[i])\n",
    "        sum_acc += accuracy_score(test_data[i][0:8],np.array(label_result[i]).tolist())\n",
    "        \n",
    "    sub_acc = sum_acc / len(test_df)\n",
    "    print(sub_acc)\n",
    "    print(\"##############################\")\n",
    "    sub_pre = 0\n",
    "    sum_pre = 0\n",
    "    num_pre = 0\n",
    "    print(\"Top1的sub-precision\")\n",
    "    for i in range(len(test_df)):\n",
    "#         print(test_data[i][0:8])\n",
    "#         print(label_result[i])\n",
    "\n",
    "        if np.sum(label_result[i]) != 0:\n",
    "            sum_pre += precision_score(test_data[i][0:8],np.array(label_result[i]).tolist(),average=None)[1]\n",
    "            num_pre += 1\n",
    "            \n",
    "    print(\"Task2的Precision的分子\",sum_pre)\n",
    "    print(\"Task2的Precision的分母\",num_pre)\n",
    "    sub_pre = sum_pre / num_pre\n",
    "    print(sub_pre)\n",
    "    print(\"##############################\")\n",
    "    sub_rec = 0\n",
    "    sum_rec = 0\n",
    "    num_rec = 0\n",
    "    print(\"Top1的sub-recall\")\n",
    "    for i in range(len(test_df)):\n",
    "#         print(test_data[i][0:8])\n",
    "#         print(label_result[i])\n",
    "\n",
    "        if sum(test_data[i][0:8]) != 0:\n",
    "            sum_rec += recall_score(test_data[i][0:8],np.array(label_result[i]).tolist(),average=None)[1]\n",
    "            num_rec += 1\n",
    "    \n",
    "    print(\"Task2的Recall的分子\",sum_rec)\n",
    "    print(\"Task2的Recall的分母\",num_rec)\n",
    "    sub_rec = sum_rec / num_rec\n",
    "    print(sub_rec)\n",
    "    print(\"##############################\")\n",
    "    \n",
    "    topk = 0\n",
    "    topk_num1 = []\n",
    "    top1_acc = 0\n",
    "    top3_acc = 0\n",
    "    top5_acc = 0\n",
    "    top10_acc = 0\n",
    "    \n",
    "    print(\"data length: \",len(test_df))\n",
    "    print(\"top 1:\")\n",
    "    for i in range(len(test_df)):\n",
    "        preresult = np.array(test_result[i]).tolist()\n",
    "        # print(test_data)\n",
    "        # print(test_result)\n",
    "        # print(np.array(test_result[i]).tolist())\n",
    "        # print(\"###############\")\n",
    "        topk += get_topK(test_data[i][0:8],preresult ,1)[0]\n",
    "        topk_num1.append(get_topK(test_data[i][0:8],preresult ,1)[1])\n",
    "#         print(get_topK(test_data[i][0:8],preresult ,1)[0])\n",
    "    print(\"sum = \",topk)\n",
    "    top1_acc = topk / len(test_df)\n",
    "    print(\"rate = \",top1_acc)\n",
    "#     print(topk_num1)\n",
    "\n",
    "    topk = 0\n",
    "    topk_num3 = []\n",
    "    print(\"##############################\")\n",
    "    print(\"top 3:\")\n",
    "    for i in range(len(test_df)):\n",
    "        preresult = np.array(test_result[i]).tolist()\n",
    "        # print(test_data)\n",
    "        # print(test_result)\n",
    "        # print(np.array(test_result[i]).tolist())\n",
    "        # print(\"###############\")\n",
    "        topk += get_topK(test_data[i][0:8],preresult ,3)[0]\n",
    "        topk_num3.append(get_topK(test_data[i][0:8],preresult ,3)[1])\n",
    "#         print(get_topK(test_data[i][0:8],preresult ,3)[0])\n",
    "    print(\"sum = \",topk)\n",
    "    top3_acc = topk / len(test_df)\n",
    "    print(\"rate = \",top3_acc)\n",
    "#     print(topk_num3)\n",
    "\n",
    "    topk = 0\n",
    "    topk_num5 = []\n",
    "    print(\"##############################\")\n",
    "    print(\"top 5:\")\n",
    "    for i in range(len(test_df)):\n",
    "        preresult = np.array(test_result[i]).tolist()\n",
    "        # print(test_data)\n",
    "        # print(test_result)\n",
    "        # print(np.array(test_result[i]).tolist())\n",
    "        # print(\"###############\")\n",
    "        topk += get_topK(test_data[i][0:8],preresult ,5)[0]\n",
    "        topk_num5.append(get_topK(test_data[i][0:8],preresult ,5)[1])\n",
    "#         print(get_topK(test_data[i][0:8],preresult ,5)[0])\n",
    "    print(\"sum = \",topk)\n",
    "    top5_acc = topk / len(test_df)\n",
    "    print(\"rate = \",top5_acc)\n",
    "#     print(topk_num5)\n",
    "\n",
    "    topk = 0\n",
    "    topk_num10 = []\n",
    "    print(\"##############################\")\n",
    "    print(\"top 10:\")\n",
    "    for i in range(len(test_df)):\n",
    "        preresult = np.array(test_result[i]).tolist()\n",
    "        # print(test_data)\n",
    "        # print(test_result)\n",
    "        # print(np.array(test_result[i]).tolist())\n",
    "        # print(\"###############\")\n",
    "        topk += get_topK(test_data[i][0:8],preresult ,10)[0]\n",
    "        topk_num10.append(get_topK(test_data[i][0:8],preresult ,10)[1])\n",
    "#         print(get_topK(test_data[i][0:8],preresult ,10)[0])\n",
    "    print(\"sum = \",topk)\n",
    "    top10_acc = topk / len(test_df)\n",
    "    print(\"rate = \",top10_acc)\n",
    "#     print(topk_num10)\n",
    "    print(\"##############################\")\n",
    "    sum_rank = 0\n",
    "    num_rank = 0\n",
    "    MRR = 0\n",
    "    for i in range(len(topk_num10)):\n",
    "        if topk_num10[i] != 0:\n",
    "            sum_rank += topk_num10[i]\n",
    "            num_rank += 1\n",
    "    print(\"top10范围下的平均排名MRR：\")\n",
    "    MRR = sum_rank / num_rank\n",
    "    print(MRR)\n",
    "    print(\"##############################\")\n",
    "    print(\"各类别缺陷分布统计:\")\n",
    "    d_num = [0,0,0,0,0,0,0,0]\n",
    "    for i in range(len(test_df)):\n",
    "        for j in range(8):\n",
    "            if test_data[i][j] == 1:\n",
    "                d_num[j] += 1 \n",
    "    print(d_num)\n",
    "    print(\"##############################\")\n",
    "    print(\"各类别缺陷预测正确率统计:\")\n",
    "    p_num = [0,0,0,0,0,0,0,0]\n",
    "    for i in range(len(test_df)):\n",
    "        preLabel = np.array(label_result[i]).tolist()    \n",
    "        for j in range(8):\n",
    "            if ((test_data[i][j] == 1) and (int(preLabel[j]) == 1)):\n",
    "                p_num[j] += 1\n",
    "    print(p_num)    \n",
    "    print(\"|||||||||||||||||||||||||||||||\")\n",
    "    print(sub_acc)\n",
    "    print(sub_pre)\n",
    "    print(sub_rec)\n",
    "    print(top1_acc)\n",
    "    print(top3_acc)\n",
    "    print(top5_acc)\n",
    "    print(top10_acc)\n",
    "    \n",
    "    for k in range(8):\n",
    "        print(d_num[k]/len(test_df))\n",
    "    for p in range(8):\n",
    "        if (d_num[p] != 0):\n",
    "            print(p_num[p]/d_num[p])\n",
    "        else:\n",
    "            print('0')\n",
    "    print(MRR)\n",
    "    print(\"=====================================================================\")\n",
    "    \n",
    "def eval3(predProb2,test_df):    \n",
    "    print(\"……………………………………………………………………………………………\")\n",
    "    print(\"……………………………………………………………………………………………\")\n",
    "    print(\"……………………………………………………………………………………………\")\n",
    "    print(\"开始进行任务3评估\")\n",
    "    print(\"###############\",software,\"###############\")\n",
    "    test_result = pd.DataFrame()\n",
    "    for i in range(len(test_df)):\n",
    "        test_result[i] =  predProb2[i]\n",
    "\n",
    "    label_result = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(test_df)):\n",
    "        label_result[i] =  trans(predProb2[i])\n",
    "   \n",
    "    test_data = np.array(test_df).tolist()\n",
    "    sub_acc = 0\n",
    "    sum_acc = 0\n",
    "    \n",
    "    print(\"Top1的sub-accuracy\")\n",
    "    for i in range(len(test_df)):\n",
    "#         print(test_data[i][0:8])\n",
    "#         print(label_result[i])\n",
    "        sum_acc += accuracy_score(test_data[i][0:20],np.array(label_result[i]).tolist())\n",
    "        \n",
    "    sub_acc = sum_acc / len(test_df)\n",
    "    print(sub_acc)\n",
    "    print(\"##############################\")\n",
    "    sub_pre = 0\n",
    "    sum_pre = 0\n",
    "    num_pre = 0\n",
    "    print(\"Top1的sub-precision\")\n",
    "    for i in range(len(test_df)):\n",
    "#         print(test_data[i][0:8])\n",
    "#         print(label_result[i])\n",
    "        if np.sum(label_result[i]) != 0:\n",
    "            sum_pre += precision_score(test_data[i][0:20],np.array(label_result[i]).tolist(),average=None)[1]\n",
    "            num_pre += 1\n",
    "            \n",
    "    print(\"Task3的Precision的分子\",sum_pre)\n",
    "    print(\"Task3的Precision的分母\",num_pre)\n",
    "    if num_pre != 0:\n",
    "        sub_pre = sum_pre / num_pre\n",
    "    else:\n",
    "        sub_pre = 0\n",
    "    print(sub_pre)\n",
    "    print(\"##############################\")\n",
    "    sub_rec = 0\n",
    "    sum_rec = 0\n",
    "    num_rec = 0\n",
    "    print(\"Top1的sub-recall\")\n",
    "    for i in range(len(test_df)):\n",
    "#         print(test_data[i][0:8])\n",
    "#         print(label_result[i])        \n",
    "        if sum(test_data[i][0:20]) != 0:\n",
    "            sum_rec += recall_score(test_data[i][0:20],np.array(label_result[i]).tolist(),average=None)[1]\n",
    "            num_rec += 1\n",
    "            \n",
    "    print(\"Task3的Recall的分子\",sum_rec)\n",
    "    print(\"Task3的Recall的分母\",num_rec)\n",
    "    sub_rec = sum_rec / num_rec\n",
    "    print(sub_rec)\n",
    "    print(\"##############################\")\n",
    "    \n",
    "    topk = 0\n",
    "    topk_num1 = []\n",
    "    top1_acc = 0\n",
    "    top3_acc = 0\n",
    "    top5_acc = 0\n",
    "    top10_acc = 0\n",
    "    \n",
    "    print(\"data length: \",len(test_df))\n",
    "    print(\"top 1:\")\n",
    "    for i in range(len(test_df)):\n",
    "        preresult = np.array(test_result[i]).tolist()\n",
    "        # print(test_data)\n",
    "        # print(test_result)\n",
    "        # print(np.array(test_result[i]).tolist())\n",
    "        # print(\"###############\")\n",
    "        topk += get_topK(test_data[i][0:20],preresult ,1)[0]\n",
    "        topk_num1.append(get_topK(test_data[i][0:20],preresult ,1)[1])\n",
    "#         print(get_topK(test_data[i][0:8],preresult ,1)[0])\n",
    "    print(\"sum = \",topk)\n",
    "    top1_acc = topk / len(test_df)\n",
    "    print(\"rate = \",top1_acc)\n",
    "#     print(topk_num1)\n",
    "\n",
    "    topk = 0\n",
    "    topk_num3 = []\n",
    "    print(\"##############################\")\n",
    "    print(\"top 3:\")\n",
    "    for i in range(len(test_df)):\n",
    "        preresult = np.array(test_result[i]).tolist()\n",
    "        # print(test_data)\n",
    "        # print(test_result)\n",
    "        # print(np.array(test_result[i]).tolist())\n",
    "        # print(\"###############\")\n",
    "        topk += get_topK(test_data[i][0:20],preresult ,3)[0]\n",
    "        topk_num3.append(get_topK(test_data[i][0:20],preresult ,3)[1])\n",
    "#         print(get_topK(test_data[i][0:8],preresult ,3)[0])\n",
    "    print(\"sum = \",topk)\n",
    "    top3_acc = topk / len(test_df)\n",
    "    print(\"rate = \",top3_acc)\n",
    "#     print(topk_num3)\n",
    "\n",
    "    topk = 0\n",
    "    topk_num5 = []\n",
    "    print(\"##############################\")\n",
    "    print(\"top 5:\")\n",
    "    for i in range(len(test_df)):\n",
    "        preresult = np.array(test_result[i]).tolist()\n",
    "        # print(test_data)\n",
    "        # print(test_result)\n",
    "        # print(np.array(test_result[i]).tolist())\n",
    "        # print(\"###############\")\n",
    "        topk += get_topK(test_data[i][0:20],preresult ,5)[0]\n",
    "        topk_num5.append(get_topK(test_data[i][0:20],preresult ,5)[1])\n",
    "#         print(get_topK(test_data[i][0:8],preresult ,5)[0])\n",
    "    print(\"sum = \",topk)\n",
    "    top5_acc = topk / len(test_df)\n",
    "    print(\"rate = \",top5_acc)\n",
    "#     print(topk_num5)\n",
    "\n",
    "    topk = 0\n",
    "    topk_num10 = []\n",
    "    print(\"##############################\")\n",
    "    print(\"top 10:\")\n",
    "    for i in range(len(test_df)):\n",
    "        preresult = np.array(test_result[i]).tolist()\n",
    "        # print(test_data)\n",
    "        # print(test_result)\n",
    "        # print(np.array(test_result[i]).tolist())\n",
    "        # print(\"###############\")\n",
    "        topk += get_topK(test_data[i][0:20],preresult ,10)[0]\n",
    "        topk_num10.append(get_topK(test_data[i][0:20],preresult ,10)[1])\n",
    "#         print(get_topK(test_data[i][0:8],preresult ,10)[0])\n",
    "    print(\"sum = \",topk)\n",
    "    top10_acc = topk / len(test_df)\n",
    "    print(\"rate = \",top10_acc)\n",
    "#     print(topk_num10)\n",
    "    print(\"##############################\")\n",
    "    sum_rank = 0\n",
    "    num_rank = 0\n",
    "    MRR = 0\n",
    "    for i in range(len(topk_num10)):\n",
    "        if topk_num10[i] != 0:\n",
    "            sum_rank += topk_num10[i]\n",
    "            num_rank += 1\n",
    "    print(\"top10范围下的平均排名MRR：\")\n",
    "    MRR = sum_rank / num_rank\n",
    "    print(MRR)\n",
    "    print(\"##############################\")\n",
    "    print(\"各类别缺陷分布统计:\")\n",
    "    d_num = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for i in range(len(test_df)):\n",
    "        for j in range(20):\n",
    "            if test_data[i][j] == 1:\n",
    "                d_num[j] += 1 \n",
    "    print(d_num)\n",
    "    print(\"##############################\")\n",
    "    print(\"各类别缺陷预测正确率统计:\")\n",
    "    p_num = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for i in range(len(test_df)):\n",
    "        preLabel = np.array(label_result[i]).tolist()    \n",
    "        for j in range(20):\n",
    "            if ((test_data[i][j] == 1) and (int(preLabel[j]) == 1)):\n",
    "                p_num[j] += 1\n",
    "    print(p_num)    \n",
    "    print(\"|||||||||||||||||||||||||||||||\")\n",
    "    print(sub_acc)\n",
    "    print(sub_pre)\n",
    "    print(sub_rec)\n",
    "    print(top1_acc)\n",
    "    print(top3_acc)\n",
    "    print(top5_acc)\n",
    "    print(top10_acc)\n",
    "    \n",
    "    for k in range(20):\n",
    "        print(d_num[k]/len(test_df))\n",
    "    for p in range(20):\n",
    "        if (d_num[p] != 0):\n",
    "            print(p_num[p]/d_num[p])\n",
    "        else:\n",
    "            print('0')\n",
    "    print(MRR)\n",
    "    print(\"=====================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:name   1:lines   2:true_lines   3:Statements   4:Percent Branch Statements   5:Percent Lines with Comments   6:Functions   7:Line Number of Most Complex Function   8:Line Number of Deepest Block   9:Maximum Block Depth   10:Average Block Depth   11:Average Complexity   \n",
      "\n",
      "0:file_path   1:total_lines   2:true_lines   3:if_bug   4:bug_0_lines   5:bug_1_lines   6:7pk_0   7:7pk_1   8:7pk_2   9:7pk_3   10:7pk_4   11:7pk_5   12:7pk_6   13:7pk_7   14:bug0   15:bug1   16:bug2   17:bug3   18:bug4   19:bug5   20:bug6   21:bug7   22:bug8   23:bug9   24:bug10   25:bug11   26:bug12   27:bug13   28:bug14   29:bug15   30:bug16   31:bug17   32:bug18   33:bug19   \n"
     ]
    }
   ],
   "source": [
    "format_of_feature = [\"name\",\"lines\", \"true_lines\",\n",
    "                     \"Statements\", \"Percent Branch Statements\", \"Percent Lines with Comments\", \"Functions\",\n",
    "                     \"Line Number of Most Complex Function\", \"Line Number of Deepest Block\",\n",
    "                     \"Maximum Block Depth\", \"Average Block Depth\", \"Average Complexity\"]\n",
    "ori_len_feature = len(format_of_feature)\n",
    "N_location = 20\n",
    "# for tool in [\"cpp\", 'ts', 'fl']:\n",
    "#     for i in range(N_location):\n",
    "#         for rank in [\"err\", 'warn']:\n",
    "#             format_of_feature.append(\"_\".join([tool, rank, str(i)]))\n",
    "\n",
    "# format_of_feature += [\"cpp_7pkd_0\",\"cpp_7pkd_1\",\"cpp_7pkd_2\",\"cpp_7pkd_3\",\"cpp_7pkd_4\",\"cpp_7pkd_5\",\"cpp_7pkd_6\",\"cpp_7pkd_7\",\"ts_7pkd_0\",\"ts_7pkd_1\",\"ts_7pkd_2\",\"ts_7pkd_3\",\"ts_7pkd_4\",\"ts_7pkd_5\",\"ts_7pkd_6\",\"ts_7pkd_7\",\"fl_7pkd_0\",\"fl_7pkd_1\",\"fl_7pkd_2\",\"fl_7pkd_3\",\"fl_7pkd_4\",\"fl_7pkd_5\",\"fl_7pkd_6\",\"fl_7pkd_7\",\"cpp_7pkn_0\",\"cpp_7pkn_1\",\"cpp_7pkn_2\",\"cpp_7pkn_3\",\"cpp_7pkn_4\",\"cpp_7pkn_5\",\"cpp_7pkn_6\",\"cpp_7pkn_7\",\"ts_7pkn_0\",\"ts_7pkn_1\",\"ts_7pkn_2\",\"ts_7pkn_3\",\"ts_7pkn_4\",\"ts_7pkn_5\",\"ts_7pkn_6\",\"ts_7pkn_7\",\"fl_7pkn_0\",\"fl_7pkn_1\",\"fl_7pkn_2\",\"fl_7pkn_3\",\"fl_7pkn_4\",\"fl_7pkn_5\",\"fl_7pkn_6\",\"fl_7pkn_7\"]            \n",
    "            \n",
    "format_of_jts = [\"file_path\", \"total_lines\", \"true_lines\", \"if_bug\", \"bug_0_lines\", \"bug_1_lines\",\n",
    "                 \"7pk_0\", \"7pk_1\", \"7pk_2\", \"7pk_3\", \"7pk_4\", \"7pk_5\", \"7pk_6\", \"7pk_7\"]\n",
    "ori_len_jts = len(format_of_jts)\n",
    "for i in range(N_location):\n",
    "    format_of_jts.append(\"bug\"+str(i))\n",
    "s1 = \"\"\n",
    "s2 = \"\"\n",
    "for i in range(len(format_of_feature)):\n",
    "    s1 += str(i) + \":\" + format_of_feature[i]+ \"   \"\n",
    "print(s1)\n",
    "print()\n",
    "for i in range(len(format_of_jts)):\n",
    "    s2 += str(i) + \":\" + format_of_jts[i]+ \"   \"\n",
    "print(s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if_bug', 'name', 'lines', 'true_lines', 'Statements', 'Percent Branch Statements', 'Percent Lines with Comments', 'Functions', 'Line Number of Most Complex Function', 'Line Number of Deepest Block', 'Maximum Block Depth', 'Average Block Depth', 'Average Complexity']\n",
      "……………………………………………………………………………………………\n",
      "['7pk_0', '7pk_1', '7pk_2', '7pk_3', '7pk_4', '7pk_5', '7pk_6', '7pk_7', 'name', 'lines', 'true_lines', 'Statements', 'Percent Branch Statements', 'Percent Lines with Comments', 'Functions', 'Line Number of Most Complex Function', 'Line Number of Deepest Block', 'Maximum Block Depth', 'Average Block Depth', 'Average Complexity']\n",
      "……………………………………………………………………………………………\n",
      "['bug0', 'bug1', 'bug2', 'bug3', 'bug4', 'bug5', 'bug6', 'bug7', 'bug8', 'bug9', 'bug10', 'bug11', 'bug12', 'bug13', 'bug14', 'bug15', 'bug16', 'bug17', 'bug18', 'bug19', 'name', 'lines', 'true_lines', 'Statements', 'Percent Branch Statements', 'Percent Lines with Comments', 'Functions', 'Line Number of Most Complex Function', 'Line Number of Deepest Block', 'Maximum Block Depth', 'Average Block Depth', 'Average Complexity']\n"
     ]
    }
   ],
   "source": [
    "#导入数据文件\n",
    "software = 'jts_java'\n",
    "method = ''\n",
    "# method = 'C_step2/'\n",
    "# method = 'JAVA_step2/'\n",
    "# 设置输出数据的格式\n",
    "pd.set_option('precision', 5) #设置精度\n",
    "\n",
    "cols1 = [format_of_jts[3]] + format_of_feature\n",
    "print(cols1)\n",
    "print(\"……………………………………………………………………………………………\")\n",
    "cols2 = format_of_jts[6:14] + format_of_feature\n",
    "print(cols2)\n",
    "print(\"……………………………………………………………………………………………\")\n",
    "cols3 = format_of_jts[14:34] + format_of_feature\n",
    "print(cols3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import TimeDistributed,RepeatVector\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, LSTM, merge,Bidirectional,Multiply\n",
    "from keras.models import Model\n",
    "\n",
    "from attention_utils import get_activations\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "\n",
    "## spatial\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "def attention_s_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = inputs\n",
    "    a = Dense(input_dim, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction1')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((1, 2),name='attention_vec1')(a)\n",
    "\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "def attention_s_block2(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = inputs\n",
    "    a = Dense(input_dim, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction2')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((1, 2),name='attention_vec2')(a)\n",
    "\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "def attention_s_block3(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = inputs\n",
    "    a = Dense(input_dim, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction3')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((1, 2),name='attention_vec3')(a)\n",
    "\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
    "    # Documentation is available online on Github at the address below.\n",
    "    # From: https://github.com/philipperemy/keras-visualize-activations\n",
    "    print('----- activations -----')\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "        if print_shape_only:\n",
    "            print(layer_activations.shape)\n",
    "        else:\n",
    "            print(layer_activations)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "此为第 5 组交叉验证数据\n",
      "……………………………………………………………………………………………\n",
      "训练集为: kfold/jts_java_5_20_step1_train.csv\n",
      "测试集为: kfold/jts_java_5_20_step1_test.csv\n",
      "设定训练集和测试集数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Programming\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "……………………………………………………………………………………………\n",
      "41573 4619\n",
      "41573 4619\n",
      "41573 4619\n",
      "……………………………………………………………………………………………\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             (None, 1, 11)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encode11 (Conv1D)               (None, 1, 256)       8704        inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encode21 (Conv1D)               (None, 1, 256)       8704        inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encode31 (Conv1D)               (None, 1, 256)       8704        inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encode12 (GRU)                  (None, 64)           61632       encode11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode22 (GRU)                  (None, 64)           61632       encode21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode32 (GRU)                  (None, 64)           61632       encode31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode13 (Dropout)              (None, 64)           0           encode12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode23 (Dropout)              (None, 64)           0           encode22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode33 (Dropout)              (None, 64)           0           encode32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode14 (RepeatVector)         (None, 1, 64)        0           encode13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode24 (RepeatVector)         (None, 1, 64)        0           encode23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode34 (RepeatVector)         (None, 1, 64)        0           encode33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode15 (Dense)                (None, 1, 32)        2080        encode14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode25 (Dense)                (None, 1, 32)        2080        encode24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode35 (Dense)                (None, 1, 32)        2080        encode34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode16 (Dropout)              (None, 1, 32)        0           encode15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode26 (Dropout)              (None, 1, 32)        0           encode25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode36 (Dropout)              (None, 1, 32)        0           encode35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode1 (Dense)                 (None, 1, 32)        1056        encode16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode2 (Dense)                 (None, 1, 32)        1056        encode26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode3 (Dense)                 (None, 1, 32)        1056        encode36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 96)        0           encode1[0][0]                    \n",
      "                                                                 encode2[0][0]                    \n",
      "                                                                 encode3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 96)        9312        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec1 (Permute)        (None, 1, 96)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1, 96)        0           concatenate_1[0][0]              \n",
      "                                                                 attention_vec1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention1 (Dense)              (None, 1, 96)        9312        multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decode11 (GRU)                  (None, 64)           30912       attention1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decode21 (GRU)                  (None, 64)           30912       attention1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decode31 (GRU)                  (None, 64)           30912       attention1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decode12 (Dense)                (None, 32)           2080        decode11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "decode22 (Dense)                (None, 32)           2080        decode21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "decode32 (Dense)                (None, 32)           2080        decode31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output1 (Dense)                 (None, 1)            33          decode12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output2 (Dense)                 (None, 8)            264         decode22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output3 (Dense)                 (None, 20)           660         decode32[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 338,973\n",
      "Trainable params: 338,973\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 37415 samples, validate on 4158 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Programming\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "37415/37415 [==============================] - 9s 245us/step - loss: 0.7014 - output1_loss: 0.3558 - output2_loss: 0.1875 - output3_loss: 0.1582 - output1_acc: 0.8468 - output2_acc: 0.9255 - output3_acc: 0.9588 - val_loss: 0.5018 - val_output1_loss: 0.2452 - val_output2_loss: 0.1437 - val_output3_loss: 0.1129 - val_output1_acc: 0.9218 - val_output2_acc: 0.9335 - val_output3_acc: 0.9603\n",
      "Epoch 2/100\n",
      "37415/37415 [==============================] - 4s 114us/step - loss: 0.5087 - output1_loss: 0.2547 - output2_loss: 0.1436 - output3_loss: 0.1104 - output1_acc: 0.9171 - output2_acc: 0.9324 - output3_acc: 0.9600 - val_loss: 0.4818 - val_output1_loss: 0.2344 - val_output2_loss: 0.1444 - val_output3_loss: 0.1030 - val_output1_acc: 0.9226 - val_output2_acc: 0.9332 - val_output3_acc: 0.9608\n",
      "Epoch 3/100\n",
      "37415/37415 [==============================] - 4s 116us/step - loss: 0.4901 - output1_loss: 0.2454 - output2_loss: 0.1425 - output3_loss: 0.1022 - output1_acc: 0.9187 - output2_acc: 0.9327 - output3_acc: 0.9605 - val_loss: 0.4686 - val_output1_loss: 0.2248 - val_output2_loss: 0.1456 - val_output3_loss: 0.0982 - val_output1_acc: 0.9264 - val_output2_acc: 0.9308 - val_output3_acc: 0.9610\n",
      "Epoch 4/100\n",
      "37415/37415 [==============================] - 4s 115us/step - loss: 0.4819 - output1_loss: 0.2423 - output2_loss: 0.1424 - output3_loss: 0.0972 - output1_acc: 0.9185 - output2_acc: 0.9322 - output3_acc: 0.9607 - val_loss: 0.4478 - val_output1_loss: 0.2137 - val_output2_loss: 0.1421 - val_output3_loss: 0.0920 - val_output1_acc: 0.9266 - val_output2_acc: 0.9325 - val_output3_acc: 0.9617\n",
      "Epoch 5/100\n",
      "37415/37415 [==============================] - 4s 115us/step - loss: 0.4692 - output1_loss: 0.2329 - output2_loss: 0.1414 - output3_loss: 0.0950 - output1_acc: 0.9201 - output2_acc: 0.9324 - output3_acc: 0.9609 - val_loss: 0.4513 - val_output1_loss: 0.2136 - val_output2_loss: 0.1427 - val_output3_loss: 0.0949 - val_output1_acc: 0.9271 - val_output2_acc: 0.9307 - val_output3_acc: 0.9612\n",
      "Epoch 6/100\n",
      "37415/37415 [==============================] - 4s 116us/step - loss: 0.4684 - output1_loss: 0.2323 - output2_loss: 0.1411 - output3_loss: 0.0951 - output1_acc: 0.9188 - output2_acc: 0.9323 - output3_acc: 0.9608 - val_loss: 0.4424 - val_output1_loss: 0.2104 - val_output2_loss: 0.1411 - val_output3_loss: 0.0910 - val_output1_acc: 0.9271 - val_output2_acc: 0.9331 - val_output3_acc: 0.9613\n",
      "Epoch 7/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.4628 - output1_loss: 0.2282 - output2_loss: 0.1403 - output3_loss: 0.0944 - output1_acc: 0.9197 - output2_acc: 0.9330 - output3_acc: 0.9609 - val_loss: 0.4421 - val_output1_loss: 0.2093 - val_output2_loss: 0.1410 - val_output3_loss: 0.0919 - val_output1_acc: 0.9274 - val_output2_acc: 0.9333 - val_output3_acc: 0.9614\n",
      "Epoch 8/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.4567 - output1_loss: 0.2232 - output2_loss: 0.1396 - output3_loss: 0.0939 - output1_acc: 0.9206 - output2_acc: 0.9337 - output3_acc: 0.9612 - val_loss: 0.4484 - val_output1_loss: 0.2127 - val_output2_loss: 0.1412 - val_output3_loss: 0.0945 - val_output1_acc: 0.9242 - val_output2_acc: 0.9347 - val_output3_acc: 0.9612\n",
      "Epoch 9/100\n",
      "37415/37415 [==============================] - 4s 116us/step - loss: 0.4459 - output1_loss: 0.2153 - output2_loss: 0.1378 - output3_loss: 0.0928 - output1_acc: 0.9214 - output2_acc: 0.9359 - output3_acc: 0.9615 - val_loss: 0.4258 - val_output1_loss: 0.1987 - val_output2_loss: 0.1384 - val_output3_loss: 0.0888 - val_output1_acc: 0.9269 - val_output2_acc: 0.9365 - val_output3_acc: 0.9630\n",
      "Epoch 10/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.4433 - output1_loss: 0.2138 - output2_loss: 0.1371 - output3_loss: 0.0924 - output1_acc: 0.9209 - output2_acc: 0.9367 - output3_acc: 0.9618 - val_loss: 0.4383 - val_output1_loss: 0.2083 - val_output2_loss: 0.1406 - val_output3_loss: 0.0894 - val_output1_acc: 0.9214 - val_output2_acc: 0.9369 - val_output3_acc: 0.9634\n",
      "Epoch 11/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.4384 - output1_loss: 0.2109 - output2_loss: 0.1365 - output3_loss: 0.0910 - output1_acc: 0.9214 - output2_acc: 0.9366 - output3_acc: 0.9620 - val_loss: 0.4180 - val_output1_loss: 0.1934 - val_output2_loss: 0.1366 - val_output3_loss: 0.0879 - val_output1_acc: 0.9266 - val_output2_acc: 0.9375 - val_output3_acc: 0.9641\n",
      "Epoch 12/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.4382 - output1_loss: 0.2112 - output2_loss: 0.1363 - output3_loss: 0.0906 - output1_acc: 0.9216 - output2_acc: 0.9366 - output3_acc: 0.9620 - val_loss: 0.4238 - val_output1_loss: 0.1970 - val_output2_loss: 0.1385 - val_output3_loss: 0.0883 - val_output1_acc: 0.9252 - val_output2_acc: 0.9354 - val_output3_acc: 0.9631\n",
      "Epoch 13/100\n",
      "37415/37415 [==============================] - 4s 116us/step - loss: 0.4294 - output1_loss: 0.2053 - output2_loss: 0.1345 - output3_loss: 0.0897 - output1_acc: 0.9221 - output2_acc: 0.9374 - output3_acc: 0.9619 - val_loss: 0.4158 - val_output1_loss: 0.1952 - val_output2_loss: 0.1347 - val_output3_loss: 0.0859 - val_output1_acc: 0.9266 - val_output2_acc: 0.9384 - val_output3_acc: 0.9636\n",
      "Epoch 14/100\n",
      "37415/37415 [==============================] - 4s 119us/step - loss: 0.4285 - output1_loss: 0.2059 - output2_loss: 0.1332 - output3_loss: 0.0894 - output1_acc: 0.9214 - output2_acc: 0.9376 - output3_acc: 0.9621 - val_loss: 0.4022 - val_output1_loss: 0.1857 - val_output2_loss: 0.1314 - val_output3_loss: 0.0850 - val_output1_acc: 0.9276 - val_output2_acc: 0.9371 - val_output3_acc: 0.9638\n",
      "Epoch 15/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.4237 - output1_loss: 0.2037 - output2_loss: 0.1316 - output3_loss: 0.0885 - output1_acc: 0.9223 - output2_acc: 0.9382 - output3_acc: 0.9622 - val_loss: 0.4131 - val_output1_loss: 0.1949 - val_output2_loss: 0.1332 - val_output3_loss: 0.0850 - val_output1_acc: 0.9262 - val_output2_acc: 0.9394 - val_output3_acc: 0.9633\n",
      "Epoch 16/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.4192 - output1_loss: 0.2024 - output2_loss: 0.1297 - output3_loss: 0.0872 - output1_acc: 0.9228 - output2_acc: 0.9396 - output3_acc: 0.9626 - val_loss: 0.3997 - val_output1_loss: 0.1873 - val_output2_loss: 0.1287 - val_output3_loss: 0.0836 - val_output1_acc: 0.9264 - val_output2_acc: 0.9404 - val_output3_acc: 0.9646\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37415/37415 [==============================] - 4s 119us/step - loss: 0.4149 - output1_loss: 0.2004 - output2_loss: 0.1278 - output3_loss: 0.0867 - output1_acc: 0.9227 - output2_acc: 0.9404 - output3_acc: 0.9626 - val_loss: 0.3969 - val_output1_loss: 0.1860 - val_output2_loss: 0.1265 - val_output3_loss: 0.0845 - val_output1_acc: 0.9269 - val_output2_acc: 0.9398 - val_output3_acc: 0.9643\n",
      "Epoch 18/100\n",
      "37415/37415 [==============================] - 4s 115us/step - loss: 0.4096 - output1_loss: 0.1975 - output2_loss: 0.1264 - output3_loss: 0.0857 - output1_acc: 0.9234 - output2_acc: 0.9409 - output3_acc: 0.9628 - val_loss: 0.3945 - val_output1_loss: 0.1833 - val_output2_loss: 0.1270 - val_output3_loss: 0.0842 - val_output1_acc: 0.9259 - val_output2_acc: 0.9411 - val_output3_acc: 0.9641\n",
      "Epoch 19/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.4077 - output1_loss: 0.1967 - output2_loss: 0.1256 - output3_loss: 0.0854 - output1_acc: 0.9228 - output2_acc: 0.9414 - output3_acc: 0.9629 - val_loss: 0.3940 - val_output1_loss: 0.1839 - val_output2_loss: 0.1271 - val_output3_loss: 0.0830 - val_output1_acc: 0.9278 - val_output2_acc: 0.9408 - val_output3_acc: 0.9642\n",
      "Epoch 20/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.4166 - output1_loss: 0.2032 - output2_loss: 0.1273 - output3_loss: 0.0860 - output1_acc: 0.9215 - output2_acc: 0.9409 - output3_acc: 0.9628 - val_loss: 0.3908 - val_output1_loss: 0.1831 - val_output2_loss: 0.1249 - val_output3_loss: 0.0828 - val_output1_acc: 0.9276 - val_output2_acc: 0.9399 - val_output3_acc: 0.9640\n",
      "Epoch 21/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.4066 - output1_loss: 0.1970 - output2_loss: 0.1250 - output3_loss: 0.0846 - output1_acc: 0.9229 - output2_acc: 0.9413 - output3_acc: 0.9632 - val_loss: 0.3809 - val_output1_loss: 0.1784 - val_output2_loss: 0.1224 - val_output3_loss: 0.0801 - val_output1_acc: 0.9264 - val_output2_acc: 0.9418 - val_output3_acc: 0.9649\n",
      "Epoch 22/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.3995 - output1_loss: 0.1933 - output2_loss: 0.1228 - output3_loss: 0.0834 - output1_acc: 0.9237 - output2_acc: 0.9426 - output3_acc: 0.9634 - val_loss: 0.3772 - val_output1_loss: 0.1753 - val_output2_loss: 0.1220 - val_output3_loss: 0.0799 - val_output1_acc: 0.9298 - val_output2_acc: 0.9436 - val_output3_acc: 0.9653\n",
      "Epoch 23/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.3991 - output1_loss: 0.1932 - output2_loss: 0.1222 - output3_loss: 0.0837 - output1_acc: 0.9234 - output2_acc: 0.9433 - output3_acc: 0.9634 - val_loss: 0.3810 - val_output1_loss: 0.1798 - val_output2_loss: 0.1199 - val_output3_loss: 0.0813 - val_output1_acc: 0.9281 - val_output2_acc: 0.9442 - val_output3_acc: 0.9639\n",
      "Epoch 24/100\n",
      "37415/37415 [==============================] - 5s 121us/step - loss: 0.3976 - output1_loss: 0.1927 - output2_loss: 0.1216 - output3_loss: 0.0833 - output1_acc: 0.9236 - output2_acc: 0.9436 - output3_acc: 0.9633 - val_loss: 0.3926 - val_output1_loss: 0.1824 - val_output2_loss: 0.1245 - val_output3_loss: 0.0857 - val_output1_acc: 0.9295 - val_output2_acc: 0.9416 - val_output3_acc: 0.9626\n",
      "Epoch 25/100\n",
      "37415/37415 [==============================] - 5s 122us/step - loss: 0.3974 - output1_loss: 0.1923 - output2_loss: 0.1224 - output3_loss: 0.0827 - output1_acc: 0.9237 - output2_acc: 0.9436 - output3_acc: 0.9635 - val_loss: 0.3855 - val_output1_loss: 0.1817 - val_output2_loss: 0.1221 - val_output3_loss: 0.0817 - val_output1_acc: 0.9283 - val_output2_acc: 0.9445 - val_output3_acc: 0.9637\n",
      "Epoch 26/100\n",
      "37415/37415 [==============================] - 4s 119us/step - loss: 0.3907 - output1_loss: 0.1902 - output2_loss: 0.1190 - output3_loss: 0.0816 - output1_acc: 0.9245 - output2_acc: 0.9456 - output3_acc: 0.9638 - val_loss: 0.3730 - val_output1_loss: 0.1795 - val_output2_loss: 0.1166 - val_output3_loss: 0.0769 - val_output1_acc: 0.9264 - val_output2_acc: 0.9463 - val_output3_acc: 0.9666\n",
      "Epoch 27/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.3860 - output1_loss: 0.1882 - output2_loss: 0.1176 - output3_loss: 0.0802 - output1_acc: 0.9246 - output2_acc: 0.9465 - output3_acc: 0.9644 - val_loss: 0.3655 - val_output1_loss: 0.1717 - val_output2_loss: 0.1166 - val_output3_loss: 0.0772 - val_output1_acc: 0.9295 - val_output2_acc: 0.9466 - val_output3_acc: 0.9661\n",
      "Epoch 28/100\n",
      "37415/37415 [==============================] - 5s 124us/step - loss: 0.3830 - output1_loss: 0.1865 - output2_loss: 0.1163 - output3_loss: 0.0802 - output1_acc: 0.9246 - output2_acc: 0.9470 - output3_acc: 0.9645 - val_loss: 0.3601 - val_output1_loss: 0.1689 - val_output2_loss: 0.1139 - val_output3_loss: 0.0773 - val_output1_acc: 0.9286 - val_output2_acc: 0.9480 - val_output3_acc: 0.9666\n",
      "Epoch 29/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.3841 - output1_loss: 0.1871 - output2_loss: 0.1163 - output3_loss: 0.0807 - output1_acc: 0.9247 - output2_acc: 0.9466 - output3_acc: 0.9643 - val_loss: 0.3876 - val_output1_loss: 0.1752 - val_output2_loss: 0.1257 - val_output3_loss: 0.0868 - val_output1_acc: 0.9295 - val_output2_acc: 0.9427 - val_output3_acc: 0.9620\n",
      "Epoch 30/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.3806 - output1_loss: 0.1860 - output2_loss: 0.1150 - output3_loss: 0.0796 - output1_acc: 0.9248 - output2_acc: 0.9479 - output3_acc: 0.9646 - val_loss: 0.3559 - val_output1_loss: 0.1666 - val_output2_loss: 0.1129 - val_output3_loss: 0.0764 - val_output1_acc: 0.9319 - val_output2_acc: 0.9488 - val_output3_acc: 0.9661\n",
      "Epoch 31/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.3763 - output1_loss: 0.1841 - output2_loss: 0.1131 - output3_loss: 0.0791 - output1_acc: 0.9246 - output2_acc: 0.9486 - output3_acc: 0.9647 - val_loss: 0.3482 - val_output1_loss: 0.1636 - val_output2_loss: 0.1097 - val_output3_loss: 0.0749 - val_output1_acc: 0.9312 - val_output2_acc: 0.9514 - val_output3_acc: 0.9666\n",
      "Epoch 32/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.3720 - output1_loss: 0.1816 - output2_loss: 0.1122 - output3_loss: 0.0781 - output1_acc: 0.9249 - output2_acc: 0.9490 - output3_acc: 0.9650 - val_loss: 0.3584 - val_output1_loss: 0.1705 - val_output2_loss: 0.1118 - val_output3_loss: 0.0761 - val_output1_acc: 0.9286 - val_output2_acc: 0.9506 - val_output3_acc: 0.9663\n",
      "Epoch 33/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.3701 - output1_loss: 0.1798 - output2_loss: 0.1121 - output3_loss: 0.0782 - output1_acc: 0.9249 - output2_acc: 0.9491 - output3_acc: 0.9649 - val_loss: 0.3501 - val_output1_loss: 0.1632 - val_output2_loss: 0.1121 - val_output3_loss: 0.0749 - val_output1_acc: 0.9322 - val_output2_acc: 0.9496 - val_output3_acc: 0.9668\n",
      "Epoch 34/100\n",
      "37415/37415 [==============================] - 4s 115us/step - loss: 0.3712 - output1_loss: 0.1804 - output2_loss: 0.1122 - output3_loss: 0.0785 - output1_acc: 0.9255 - output2_acc: 0.9491 - output3_acc: 0.9649 - val_loss: 0.3448 - val_output1_loss: 0.1603 - val_output2_loss: 0.1103 - val_output3_loss: 0.0742 - val_output1_acc: 0.9288 - val_output2_acc: 0.9503 - val_output3_acc: 0.9668\n",
      "Epoch 35/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.3675 - output1_loss: 0.1782 - output2_loss: 0.1118 - output3_loss: 0.0775 - output1_acc: 0.9258 - output2_acc: 0.9494 - output3_acc: 0.9651 - val_loss: 0.3409 - val_output1_loss: 0.1596 - val_output2_loss: 0.1086 - val_output3_loss: 0.0728 - val_output1_acc: 0.9310 - val_output2_acc: 0.9504 - val_output3_acc: 0.9678\n",
      "Epoch 36/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.3632 - output1_loss: 0.1747 - output2_loss: 0.1117 - output3_loss: 0.0767 - output1_acc: 0.9270 - output2_acc: 0.9494 - output3_acc: 0.9653 - val_loss: 0.3411 - val_output1_loss: 0.1597 - val_output2_loss: 0.1090 - val_output3_loss: 0.0724 - val_output1_acc: 0.9334 - val_output2_acc: 0.9506 - val_output3_acc: 0.9678\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37415/37415 [==============================] - 4s 119us/step - loss: 0.3621 - output1_loss: 0.1750 - output2_loss: 0.1105 - output3_loss: 0.0766 - output1_acc: 0.9272 - output2_acc: 0.9502 - output3_acc: 0.9654 - val_loss: 0.3320 - val_output1_loss: 0.1519 - val_output2_loss: 0.1078 - val_output3_loss: 0.0722 - val_output1_acc: 0.9329 - val_output2_acc: 0.9520 - val_output3_acc: 0.9676\n",
      "Epoch 38/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.3525 - output1_loss: 0.1699 - output2_loss: 0.1077 - output3_loss: 0.0749 - output1_acc: 0.9282 - output2_acc: 0.9516 - output3_acc: 0.9661 - val_loss: 0.3305 - val_output1_loss: 0.1493 - val_output2_loss: 0.1084 - val_output3_loss: 0.0728 - val_output1_acc: 0.9360 - val_output2_acc: 0.9525 - val_output3_acc: 0.9677\n",
      "Epoch 39/100\n",
      "37415/37415 [==============================] - 4s 120us/step - loss: 0.3570 - output1_loss: 0.1718 - output2_loss: 0.1095 - output3_loss: 0.0758 - output1_acc: 0.9280 - output2_acc: 0.9508 - output3_acc: 0.9656 - val_loss: 0.3375 - val_output1_loss: 0.1515 - val_output2_loss: 0.1105 - val_output3_loss: 0.0755 - val_output1_acc: 0.9348 - val_output2_acc: 0.9508 - val_output3_acc: 0.9667\n",
      "Epoch 40/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.3499 - output1_loss: 0.1689 - output2_loss: 0.1064 - output3_loss: 0.0745 - output1_acc: 0.9282 - output2_acc: 0.9524 - output3_acc: 0.9662 - val_loss: 0.3467 - val_output1_loss: 0.1624 - val_output2_loss: 0.1105 - val_output3_loss: 0.0738 - val_output1_acc: 0.9317 - val_output2_acc: 0.9505 - val_output3_acc: 0.9666\n",
      "Epoch 41/100\n",
      "37415/37415 [==============================] - 4s 116us/step - loss: 0.3484 - output1_loss: 0.1685 - output2_loss: 0.1055 - output3_loss: 0.0744 - output1_acc: 0.9277 - output2_acc: 0.9528 - output3_acc: 0.9662 - val_loss: 0.3394 - val_output1_loss: 0.1601 - val_output2_loss: 0.1064 - val_output3_loss: 0.0729 - val_output1_acc: 0.9322 - val_output2_acc: 0.9527 - val_output3_acc: 0.9676\n",
      "Epoch 42/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.3450 - output1_loss: 0.1662 - output2_loss: 0.1048 - output3_loss: 0.0740 - output1_acc: 0.9290 - output2_acc: 0.9534 - output3_acc: 0.9664 - val_loss: 0.3310 - val_output1_loss: 0.1559 - val_output2_loss: 0.1032 - val_output3_loss: 0.0719 - val_output1_acc: 0.9336 - val_output2_acc: 0.9541 - val_output3_acc: 0.9673\n",
      "Epoch 43/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.3418 - output1_loss: 0.1649 - output2_loss: 0.1037 - output3_loss: 0.0732 - output1_acc: 0.9297 - output2_acc: 0.9539 - output3_acc: 0.9667 - val_loss: 0.3218 - val_output1_loss: 0.1503 - val_output2_loss: 0.1016 - val_output3_loss: 0.0699 - val_output1_acc: 0.9348 - val_output2_acc: 0.9553 - val_output3_acc: 0.9691\n",
      "Epoch 44/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.3455 - output1_loss: 0.1662 - output2_loss: 0.1055 - output3_loss: 0.0738 - output1_acc: 0.9286 - output2_acc: 0.9535 - output3_acc: 0.9664 - val_loss: 0.3308 - val_output1_loss: 0.1553 - val_output2_loss: 0.1039 - val_output3_loss: 0.0716 - val_output1_acc: 0.9336 - val_output2_acc: 0.9537 - val_output3_acc: 0.9680\n",
      "Epoch 45/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.3423 - output1_loss: 0.1641 - output2_loss: 0.1042 - output3_loss: 0.0739 - output1_acc: 0.9295 - output2_acc: 0.9537 - output3_acc: 0.9663 - val_loss: 0.3312 - val_output1_loss: 0.1565 - val_output2_loss: 0.1027 - val_output3_loss: 0.0720 - val_output1_acc: 0.9334 - val_output2_acc: 0.9551 - val_output3_acc: 0.9672\n",
      "Epoch 46/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.3368 - output1_loss: 0.1621 - output2_loss: 0.1023 - output3_loss: 0.0724 - output1_acc: 0.9292 - output2_acc: 0.9544 - output3_acc: 0.9672 - val_loss: 0.3171 - val_output1_loss: 0.1463 - val_output2_loss: 0.1018 - val_output3_loss: 0.0690 - val_output1_acc: 0.9353 - val_output2_acc: 0.9548 - val_output3_acc: 0.9687\n",
      "Epoch 47/100\n",
      "37415/37415 [==============================] - 5s 124us/step - loss: 0.3374 - output1_loss: 0.1622 - output2_loss: 0.1025 - output3_loss: 0.0727 - output1_acc: 0.9294 - output2_acc: 0.9545 - output3_acc: 0.9670 - val_loss: 0.3152 - val_output1_loss: 0.1443 - val_output2_loss: 0.1018 - val_output3_loss: 0.0690 - val_output1_acc: 0.9370 - val_output2_acc: 0.9551 - val_output3_acc: 0.9703\n",
      "Epoch 48/100\n",
      "37415/37415 [==============================] - 4s 119us/step - loss: 0.3371 - output1_loss: 0.1619 - output2_loss: 0.1026 - output3_loss: 0.0726 - output1_acc: 0.9291 - output2_acc: 0.9546 - output3_acc: 0.9672 - val_loss: 0.3190 - val_output1_loss: 0.1486 - val_output2_loss: 0.1009 - val_output3_loss: 0.0695 - val_output1_acc: 0.9355 - val_output2_acc: 0.9551 - val_output3_acc: 0.9696\n",
      "Epoch 49/100\n",
      "37415/37415 [==============================] - 5s 122us/step - loss: 0.3333 - output1_loss: 0.1599 - output2_loss: 0.1013 - output3_loss: 0.0721 - output1_acc: 0.9306 - output2_acc: 0.9550 - output3_acc: 0.9673 - val_loss: 0.3139 - val_output1_loss: 0.1459 - val_output2_loss: 0.0990 - val_output3_loss: 0.0690 - val_output1_acc: 0.9367 - val_output2_acc: 0.9571 - val_output3_acc: 0.9693\n",
      "Epoch 50/100\n",
      "37415/37415 [==============================] - 5s 125us/step - loss: 0.3310 - output1_loss: 0.1580 - output2_loss: 0.1008 - output3_loss: 0.0721 - output1_acc: 0.9304 - output2_acc: 0.9552 - output3_acc: 0.9674 - val_loss: 0.3239 - val_output1_loss: 0.1532 - val_output2_loss: 0.1019 - val_output3_loss: 0.0688 - val_output1_acc: 0.9355 - val_output2_acc: 0.9554 - val_output3_acc: 0.9693\n",
      "Epoch 51/100\n",
      "37415/37415 [==============================] - 5s 124us/step - loss: 0.3323 - output1_loss: 0.1590 - output2_loss: 0.1015 - output3_loss: 0.0718 - output1_acc: 0.9294 - output2_acc: 0.9548 - output3_acc: 0.9676 - val_loss: 0.3091 - val_output1_loss: 0.1418 - val_output2_loss: 0.1002 - val_output3_loss: 0.0671 - val_output1_acc: 0.9351 - val_output2_acc: 0.9564 - val_output3_acc: 0.9710\n",
      "Epoch 52/100\n",
      "37415/37415 [==============================] - 5s 128us/step - loss: 0.3270 - output1_loss: 0.1555 - output2_loss: 0.1003 - output3_loss: 0.0712 - output1_acc: 0.9302 - output2_acc: 0.9557 - output3_acc: 0.9679 - val_loss: 0.3110 - val_output1_loss: 0.1432 - val_output2_loss: 0.0999 - val_output3_loss: 0.0678 - val_output1_acc: 0.9360 - val_output2_acc: 0.9559 - val_output3_acc: 0.9700\n",
      "Epoch 53/100\n",
      "37415/37415 [==============================] - 4s 119us/step - loss: 0.3254 - output1_loss: 0.1555 - output2_loss: 0.0988 - output3_loss: 0.0711 - output1_acc: 0.9310 - output2_acc: 0.9564 - output3_acc: 0.9676 - val_loss: 0.3210 - val_output1_loss: 0.1490 - val_output2_loss: 0.1014 - val_output3_loss: 0.0707 - val_output1_acc: 0.9334 - val_output2_acc: 0.9547 - val_output3_acc: 0.9687\n",
      "Epoch 54/100\n",
      "37415/37415 [==============================] - 5s 121us/step - loss: 0.3298 - output1_loss: 0.1580 - output2_loss: 0.1000 - output3_loss: 0.0718 - output1_acc: 0.9300 - output2_acc: 0.9558 - output3_acc: 0.9675 - val_loss: 0.3088 - val_output1_loss: 0.1443 - val_output2_loss: 0.0971 - val_output3_loss: 0.0674 - val_output1_acc: 0.9353 - val_output2_acc: 0.9581 - val_output3_acc: 0.9698\n",
      "Epoch 55/100\n",
      "37415/37415 [==============================] - 5s 121us/step - loss: 0.3258 - output1_loss: 0.1554 - output2_loss: 0.0992 - output3_loss: 0.0712 - output1_acc: 0.9302 - output2_acc: 0.9560 - output3_acc: 0.9677 - val_loss: 0.3088 - val_output1_loss: 0.1453 - val_output2_loss: 0.0964 - val_output3_loss: 0.0672 - val_output1_acc: 0.9348 - val_output2_acc: 0.9579 - val_output3_acc: 0.9703\n",
      "Epoch 56/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.3240 - output1_loss: 0.1548 - output2_loss: 0.0985 - output3_loss: 0.0707 - output1_acc: 0.9310 - output2_acc: 0.9568 - output3_acc: 0.9682 - val_loss: 0.3027 - val_output1_loss: 0.1379 - val_output2_loss: 0.0980 - val_output3_loss: 0.0667 - val_output1_acc: 0.9363 - val_output2_acc: 0.9562 - val_output3_acc: 0.9704\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37415/37415 [==============================] - 5s 121us/step - loss: 0.3206 - output1_loss: 0.1533 - output2_loss: 0.0974 - output3_loss: 0.0699 - output1_acc: 0.9310 - output2_acc: 0.9571 - output3_acc: 0.9683 - val_loss: 0.3129 - val_output1_loss: 0.1463 - val_output2_loss: 0.0988 - val_output3_loss: 0.0678 - val_output1_acc: 0.9353 - val_output2_acc: 0.9572 - val_output3_acc: 0.9704\n",
      "Epoch 58/100\n",
      "37415/37415 [==============================] - 5s 130us/step - loss: 0.3207 - output1_loss: 0.1531 - output2_loss: 0.0975 - output3_loss: 0.0701 - output1_acc: 0.9315 - output2_acc: 0.9572 - output3_acc: 0.9683 - val_loss: 0.3040 - val_output1_loss: 0.1412 - val_output2_loss: 0.0972 - val_output3_loss: 0.0657 - val_output1_acc: 0.9389 - val_output2_acc: 0.9584 - val_output3_acc: 0.9708\n",
      "Epoch 59/100\n",
      "37415/37415 [==============================] - 5s 124us/step - loss: 0.3205 - output1_loss: 0.1533 - output2_loss: 0.0972 - output3_loss: 0.0701 - output1_acc: 0.9312 - output2_acc: 0.9575 - output3_acc: 0.9684 - val_loss: 0.3016 - val_output1_loss: 0.1414 - val_output2_loss: 0.0943 - val_output3_loss: 0.0659 - val_output1_acc: 0.9353 - val_output2_acc: 0.9595 - val_output3_acc: 0.9709\n",
      "Epoch 60/100\n",
      "37415/37415 [==============================] - 5s 129us/step - loss: 0.3188 - output1_loss: 0.1523 - output2_loss: 0.0968 - output3_loss: 0.0697 - output1_acc: 0.9320 - output2_acc: 0.9578 - output3_acc: 0.9687 - val_loss: 0.3013 - val_output1_loss: 0.1410 - val_output2_loss: 0.0954 - val_output3_loss: 0.0650 - val_output1_acc: 0.9348 - val_output2_acc: 0.9574 - val_output3_acc: 0.9708\n",
      "Epoch 61/100\n",
      "37415/37415 [==============================] - 5s 125us/step - loss: 0.3202 - output1_loss: 0.1533 - output2_loss: 0.0972 - output3_loss: 0.0697 - output1_acc: 0.9316 - output2_acc: 0.9577 - output3_acc: 0.9687 - val_loss: 0.3021 - val_output1_loss: 0.1430 - val_output2_loss: 0.0938 - val_output3_loss: 0.0653 - val_output1_acc: 0.9353 - val_output2_acc: 0.9591 - val_output3_acc: 0.9714\n",
      "Epoch 62/100\n",
      "37415/37415 [==============================] - 5s 121us/step - loss: 0.3291 - output1_loss: 0.1584 - output2_loss: 0.0998 - output3_loss: 0.0709 - output1_acc: 0.9300 - output2_acc: 0.9566 - output3_acc: 0.9680 - val_loss: 0.3236 - val_output1_loss: 0.1504 - val_output2_loss: 0.1019 - val_output3_loss: 0.0713 - val_output1_acc: 0.9329 - val_output2_acc: 0.9539 - val_output3_acc: 0.9676\n",
      "Epoch 63/100\n",
      "37415/37415 [==============================] - 5s 126us/step - loss: 0.3166 - output1_loss: 0.1511 - output2_loss: 0.0965 - output3_loss: 0.0690 - output1_acc: 0.9318 - output2_acc: 0.9580 - output3_acc: 0.9686 - val_loss: 0.3090 - val_output1_loss: 0.1453 - val_output2_loss: 0.0982 - val_output3_loss: 0.0655 - val_output1_acc: 0.9380 - val_output2_acc: 0.9553 - val_output3_acc: 0.9708\n",
      "Epoch 64/100\n",
      "37415/37415 [==============================] - 5s 125us/step - loss: 0.3136 - output1_loss: 0.1496 - output2_loss: 0.0956 - output3_loss: 0.0683 - output1_acc: 0.9326 - output2_acc: 0.9579 - output3_acc: 0.9691 - val_loss: 0.3042 - val_output1_loss: 0.1428 - val_output2_loss: 0.0957 - val_output3_loss: 0.0656 - val_output1_acc: 0.9377 - val_output2_acc: 0.9575 - val_output3_acc: 0.9708\n",
      "Epoch 65/100\n",
      "37415/37415 [==============================] - 5s 131us/step - loss: 0.3192 - output1_loss: 0.1517 - output2_loss: 0.0974 - output3_loss: 0.0702 - output1_acc: 0.9325 - output2_acc: 0.9573 - output3_acc: 0.9683 - val_loss: 0.2940 - val_output1_loss: 0.1365 - val_output2_loss: 0.0929 - val_output3_loss: 0.0646 - val_output1_acc: 0.9389 - val_output2_acc: 0.9600 - val_output3_acc: 0.9711\n",
      "Epoch 66/100\n",
      "37415/37415 [==============================] - 5s 121us/step - loss: 0.3177 - output1_loss: 0.1513 - output2_loss: 0.0968 - output3_loss: 0.0696 - output1_acc: 0.9325 - output2_acc: 0.9575 - output3_acc: 0.9684 - val_loss: 0.2984 - val_output1_loss: 0.1389 - val_output2_loss: 0.0949 - val_output3_loss: 0.0646 - val_output1_acc: 0.9380 - val_output2_acc: 0.9589 - val_output3_acc: 0.9718\n",
      "Epoch 67/100\n",
      "37415/37415 [==============================] - 5s 126us/step - loss: 0.3086 - output1_loss: 0.1469 - output2_loss: 0.0943 - output3_loss: 0.0674 - output1_acc: 0.9338 - output2_acc: 0.9585 - output3_acc: 0.9694 - val_loss: 0.3105 - val_output1_loss: 0.1485 - val_output2_loss: 0.0966 - val_output3_loss: 0.0655 - val_output1_acc: 0.9363 - val_output2_acc: 0.9569 - val_output3_acc: 0.9699\n",
      "Epoch 68/100\n",
      "37415/37415 [==============================] - 5s 120us/step - loss: 0.3116 - output1_loss: 0.1483 - output2_loss: 0.0953 - output3_loss: 0.0680 - output1_acc: 0.9331 - output2_acc: 0.9579 - output3_acc: 0.9689 - val_loss: 0.2996 - val_output1_loss: 0.1375 - val_output2_loss: 0.0964 - val_output3_loss: 0.0657 - val_output1_acc: 0.9365 - val_output2_acc: 0.9578 - val_output3_acc: 0.9706\n",
      "Epoch 69/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.3104 - output1_loss: 0.1485 - output2_loss: 0.0946 - output3_loss: 0.0674 - output1_acc: 0.9337 - output2_acc: 0.9585 - output3_acc: 0.9695 - val_loss: 0.3052 - val_output1_loss: 0.1423 - val_output2_loss: 0.0966 - val_output3_loss: 0.0662 - val_output1_acc: 0.9346 - val_output2_acc: 0.9573 - val_output3_acc: 0.9699\n",
      "Epoch 70/100\n",
      "37415/37415 [==============================] - 4s 119us/step - loss: 0.3069 - output1_loss: 0.1457 - output2_loss: 0.0942 - output3_loss: 0.0670 - output1_acc: 0.9342 - output2_acc: 0.9585 - output3_acc: 0.9694 - val_loss: 0.3057 - val_output1_loss: 0.1444 - val_output2_loss: 0.0955 - val_output3_loss: 0.0658 - val_output1_acc: 0.9343 - val_output2_acc: 0.9579 - val_output3_acc: 0.9704\n",
      "Epoch 71/100\n",
      "37415/37415 [==============================] - 4s 117us/step - loss: 0.3079 - output1_loss: 0.1466 - output2_loss: 0.0943 - output3_loss: 0.0671 - output1_acc: 0.9342 - output2_acc: 0.9585 - output3_acc: 0.9695 - val_loss: 0.2892 - val_output1_loss: 0.1343 - val_output2_loss: 0.0919 - val_output3_loss: 0.0630 - val_output1_acc: 0.9363 - val_output2_acc: 0.9594 - val_output3_acc: 0.9717\n",
      "Epoch 72/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.3073 - output1_loss: 0.1473 - output2_loss: 0.0935 - output3_loss: 0.0666 - output1_acc: 0.9341 - output2_acc: 0.9589 - output3_acc: 0.9697 - val_loss: 0.3035 - val_output1_loss: 0.1418 - val_output2_loss: 0.0961 - val_output3_loss: 0.0655 - val_output1_acc: 0.9396 - val_output2_acc: 0.9578 - val_output3_acc: 0.9707\n",
      "Epoch 73/100\n",
      "37415/37415 [==============================] - 4s 119us/step - loss: 0.3052 - output1_loss: 0.1456 - output2_loss: 0.0933 - output3_loss: 0.0664 - output1_acc: 0.9349 - output2_acc: 0.9590 - output3_acc: 0.9698 - val_loss: 0.2884 - val_output1_loss: 0.1345 - val_output2_loss: 0.0920 - val_output3_loss: 0.0619 - val_output1_acc: 0.9406 - val_output2_acc: 0.9597 - val_output3_acc: 0.9729\n",
      "Epoch 74/100\n",
      "37415/37415 [==============================] - 5s 121us/step - loss: 0.3046 - output1_loss: 0.1452 - output2_loss: 0.0933 - output3_loss: 0.0661 - output1_acc: 0.9339 - output2_acc: 0.9587 - output3_acc: 0.9699 - val_loss: 0.2901 - val_output1_loss: 0.1335 - val_output2_loss: 0.0936 - val_output3_loss: 0.0630 - val_output1_acc: 0.9401 - val_output2_acc: 0.9588 - val_output3_acc: 0.9720\n",
      "Epoch 75/100\n",
      "37415/37415 [==============================] - 5s 126us/step - loss: 0.3139 - output1_loss: 0.1511 - output2_loss: 0.0946 - output3_loss: 0.0681 - output1_acc: 0.9325 - output2_acc: 0.9587 - output3_acc: 0.9692 - val_loss: 0.2866 - val_output1_loss: 0.1333 - val_output2_loss: 0.0908 - val_output3_loss: 0.0624 - val_output1_acc: 0.9387 - val_output2_acc: 0.9604 - val_output3_acc: 0.9728\n",
      "Epoch 76/100\n",
      "37415/37415 [==============================] - 5s 125us/step - loss: 0.3024 - output1_loss: 0.1441 - output2_loss: 0.0923 - output3_loss: 0.0661 - output1_acc: 0.9351 - output2_acc: 0.9598 - output3_acc: 0.9699 - val_loss: 0.3090 - val_output1_loss: 0.1426 - val_output2_loss: 0.0974 - val_output3_loss: 0.0690 - val_output1_acc: 0.9363 - val_output2_acc: 0.9570 - val_output3_acc: 0.9697\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37415/37415 [==============================] - 5s 130us/step - loss: 0.2997 - output1_loss: 0.1422 - output2_loss: 0.0919 - output3_loss: 0.0655 - output1_acc: 0.9353 - output2_acc: 0.9596 - output3_acc: 0.9703 - val_loss: 0.2883 - val_output1_loss: 0.1338 - val_output2_loss: 0.0916 - val_output3_loss: 0.0629 - val_output1_acc: 0.9367 - val_output2_acc: 0.9603 - val_output3_acc: 0.9725\n",
      "Epoch 78/100\n",
      "37415/37415 [==============================] - 5s 123us/step - loss: 0.3006 - output1_loss: 0.1438 - output2_loss: 0.0915 - output3_loss: 0.0653 - output1_acc: 0.9351 - output2_acc: 0.9600 - output3_acc: 0.9706 - val_loss: 0.2813 - val_output1_loss: 0.1286 - val_output2_loss: 0.0910 - val_output3_loss: 0.0617 - val_output1_acc: 0.9420 - val_output2_acc: 0.9607 - val_output3_acc: 0.9732\n",
      "Epoch 79/100\n",
      "37415/37415 [==============================] - 5s 122us/step - loss: 0.2959 - output1_loss: 0.1413 - output2_loss: 0.0904 - output3_loss: 0.0642 - output1_acc: 0.9347 - output2_acc: 0.9604 - output3_acc: 0.9708 - val_loss: 0.2976 - val_output1_loss: 0.1392 - val_output2_loss: 0.0939 - val_output3_loss: 0.0645 - val_output1_acc: 0.9348 - val_output2_acc: 0.9594 - val_output3_acc: 0.9715\n",
      "Epoch 80/100\n",
      "37415/37415 [==============================] - 5s 128us/step - loss: 0.2983 - output1_loss: 0.1424 - output2_loss: 0.0910 - output3_loss: 0.0649 - output1_acc: 0.9357 - output2_acc: 0.9602 - output3_acc: 0.9703 - val_loss: 0.2818 - val_output1_loss: 0.1316 - val_output2_loss: 0.0902 - val_output3_loss: 0.0600 - val_output1_acc: 0.9384 - val_output2_acc: 0.9607 - val_output3_acc: 0.9737\n",
      "Epoch 81/100\n",
      "37415/37415 [==============================] - 5s 120us/step - loss: 0.3025 - output1_loss: 0.1448 - output2_loss: 0.0919 - output3_loss: 0.0658 - output1_acc: 0.9358 - output2_acc: 0.9601 - output3_acc: 0.9702 - val_loss: 0.2939 - val_output1_loss: 0.1355 - val_output2_loss: 0.0945 - val_output3_loss: 0.0640 - val_output1_acc: 0.9380 - val_output2_acc: 0.9581 - val_output3_acc: 0.9715\n",
      "Epoch 82/100\n",
      "37415/37415 [==============================] - 4s 120us/step - loss: 0.2936 - output1_loss: 0.1404 - output2_loss: 0.0894 - output3_loss: 0.0639 - output1_acc: 0.9359 - output2_acc: 0.9607 - output3_acc: 0.9708 - val_loss: 0.2821 - val_output1_loss: 0.1324 - val_output2_loss: 0.0879 - val_output3_loss: 0.0617 - val_output1_acc: 0.9387 - val_output2_acc: 0.9618 - val_output3_acc: 0.9726\n",
      "Epoch 83/100\n",
      "37415/37415 [==============================] - 5s 120us/step - loss: 0.2952 - output1_loss: 0.1415 - output2_loss: 0.0893 - output3_loss: 0.0644 - output1_acc: 0.9360 - output2_acc: 0.9609 - output3_acc: 0.9706 - val_loss: 0.2836 - val_output1_loss: 0.1334 - val_output2_loss: 0.0903 - val_output3_loss: 0.0600 - val_output1_acc: 0.9372 - val_output2_acc: 0.9616 - val_output3_acc: 0.9741\n",
      "Epoch 84/100\n",
      "37415/37415 [==============================] - 5s 123us/step - loss: 0.2953 - output1_loss: 0.1423 - output2_loss: 0.0892 - output3_loss: 0.0638 - output1_acc: 0.9352 - output2_acc: 0.9607 - output3_acc: 0.9709 - val_loss: 0.2929 - val_output1_loss: 0.1409 - val_output2_loss: 0.0911 - val_output3_loss: 0.0610 - val_output1_acc: 0.9372 - val_output2_acc: 0.9603 - val_output3_acc: 0.9726\n",
      "Epoch 85/100\n",
      "37415/37415 [==============================] - 5s 125us/step - loss: 0.2936 - output1_loss: 0.1412 - output2_loss: 0.0887 - output3_loss: 0.0637 - output1_acc: 0.9361 - output2_acc: 0.9611 - output3_acc: 0.9712 - val_loss: 0.2755 - val_output1_loss: 0.1270 - val_output2_loss: 0.0895 - val_output3_loss: 0.0590 - val_output1_acc: 0.9389 - val_output2_acc: 0.9604 - val_output3_acc: 0.9744\n",
      "Epoch 86/100\n",
      "37415/37415 [==============================] - 5s 124us/step - loss: 0.2963 - output1_loss: 0.1419 - output2_loss: 0.0905 - output3_loss: 0.0639 - output1_acc: 0.9347 - output2_acc: 0.9603 - output3_acc: 0.9710 - val_loss: 0.2838 - val_output1_loss: 0.1317 - val_output2_loss: 0.0908 - val_output3_loss: 0.0613 - val_output1_acc: 0.9367 - val_output2_acc: 0.9602 - val_output3_acc: 0.9726\n",
      "Epoch 87/100\n",
      "37415/37415 [==============================] - 4s 120us/step - loss: 0.2925 - output1_loss: 0.1411 - output2_loss: 0.0886 - output3_loss: 0.0628 - output1_acc: 0.9355 - output2_acc: 0.9610 - output3_acc: 0.9715 - val_loss: 0.2882 - val_output1_loss: 0.1372 - val_output2_loss: 0.0908 - val_output3_loss: 0.0602 - val_output1_acc: 0.9339 - val_output2_acc: 0.9595 - val_output3_acc: 0.9734\n",
      "Epoch 88/100\n",
      "37415/37415 [==============================] - 5s 123us/step - loss: 0.2948 - output1_loss: 0.1416 - output2_loss: 0.0898 - output3_loss: 0.0633 - output1_acc: 0.9357 - output2_acc: 0.9609 - output3_acc: 0.9713 - val_loss: 0.2858 - val_output1_loss: 0.1322 - val_output2_loss: 0.0902 - val_output3_loss: 0.0633 - val_output1_acc: 0.9380 - val_output2_acc: 0.9605 - val_output3_acc: 0.9714\n",
      "Epoch 89/100\n",
      "37415/37415 [==============================] - 5s 129us/step - loss: 0.2903 - output1_loss: 0.1390 - output2_loss: 0.0883 - output3_loss: 0.0629 - output1_acc: 0.9371 - output2_acc: 0.9619 - output3_acc: 0.9714 - val_loss: 0.2970 - val_output1_loss: 0.1357 - val_output2_loss: 0.0937 - val_output3_loss: 0.0676 - val_output1_acc: 0.9384 - val_output2_acc: 0.9594 - val_output3_acc: 0.9690\n",
      "Epoch 90/100\n",
      "37415/37415 [==============================] - 5s 123us/step - loss: 0.2887 - output1_loss: 0.1387 - output2_loss: 0.0877 - output3_loss: 0.0623 - output1_acc: 0.9370 - output2_acc: 0.9619 - output3_acc: 0.9719 - val_loss: 0.2810 - val_output1_loss: 0.1360 - val_output2_loss: 0.0874 - val_output3_loss: 0.0576 - val_output1_acc: 0.9348 - val_output2_acc: 0.9604 - val_output3_acc: 0.9738\n",
      "Epoch 91/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.2907 - output1_loss: 0.1398 - output2_loss: 0.0882 - output3_loss: 0.0628 - output1_acc: 0.9370 - output2_acc: 0.9615 - output3_acc: 0.9717 - val_loss: 0.2813 - val_output1_loss: 0.1341 - val_output2_loss: 0.0878 - val_output3_loss: 0.0594 - val_output1_acc: 0.9380 - val_output2_acc: 0.9615 - val_output3_acc: 0.9733\n",
      "Epoch 92/100\n",
      "37415/37415 [==============================] - 5s 120us/step - loss: 0.2947 - output1_loss: 0.1441 - output2_loss: 0.0880 - output3_loss: 0.0626 - output1_acc: 0.9356 - output2_acc: 0.9612 - output3_acc: 0.9718 - val_loss: 0.2679 - val_output1_loss: 0.1253 - val_output2_loss: 0.0856 - val_output3_loss: 0.0571 - val_output1_acc: 0.9425 - val_output2_acc: 0.9620 - val_output3_acc: 0.9746\n",
      "Epoch 93/100\n",
      "37415/37415 [==============================] - 5s 124us/step - loss: 0.2869 - output1_loss: 0.1387 - output2_loss: 0.0867 - output3_loss: 0.0616 - output1_acc: 0.9365 - output2_acc: 0.9623 - output3_acc: 0.9720 - val_loss: 0.2726 - val_output1_loss: 0.1297 - val_output2_loss: 0.0855 - val_output3_loss: 0.0574 - val_output1_acc: 0.9406 - val_output2_acc: 0.9625 - val_output3_acc: 0.9744\n",
      "Epoch 94/100\n",
      "37415/37415 [==============================] - 5s 125us/step - loss: 0.2850 - output1_loss: 0.1371 - output2_loss: 0.0860 - output3_loss: 0.0619 - output1_acc: 0.9365 - output2_acc: 0.9624 - output3_acc: 0.9719 - val_loss: 0.2726 - val_output1_loss: 0.1264 - val_output2_loss: 0.0887 - val_output3_loss: 0.0575 - val_output1_acc: 0.9401 - val_output2_acc: 0.9607 - val_output3_acc: 0.9742\n",
      "Epoch 95/100\n",
      "37415/37415 [==============================] - 5s 124us/step - loss: 0.2886 - output1_loss: 0.1395 - output2_loss: 0.0873 - output3_loss: 0.0617 - output1_acc: 0.9354 - output2_acc: 0.9620 - output3_acc: 0.9723 - val_loss: 0.2828 - val_output1_loss: 0.1318 - val_output2_loss: 0.0906 - val_output3_loss: 0.0604 - val_output1_acc: 0.9411 - val_output2_acc: 0.9612 - val_output3_acc: 0.9734\n",
      "Epoch 96/100\n",
      "37415/37415 [==============================] - 5s 122us/step - loss: 0.2877 - output1_loss: 0.1397 - output2_loss: 0.0867 - output3_loss: 0.0613 - output1_acc: 0.9358 - output2_acc: 0.9620 - output3_acc: 0.9722 - val_loss: 0.2743 - val_output1_loss: 0.1294 - val_output2_loss: 0.0865 - val_output3_loss: 0.0584 - val_output1_acc: 0.9389 - val_output2_acc: 0.9620 - val_output3_acc: 0.9739\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37415/37415 [==============================] - 5s 122us/step - loss: 0.2877 - output1_loss: 0.1392 - output2_loss: 0.0868 - output3_loss: 0.0617 - output1_acc: 0.9357 - output2_acc: 0.9623 - output3_acc: 0.9721 - val_loss: 0.2797 - val_output1_loss: 0.1312 - val_output2_loss: 0.0902 - val_output3_loss: 0.0583 - val_output1_acc: 0.9399 - val_output2_acc: 0.9608 - val_output3_acc: 0.9748\n",
      "Epoch 98/100\n",
      "37415/37415 [==============================] - 5s 122us/step - loss: 0.2825 - output1_loss: 0.1365 - output2_loss: 0.0856 - output3_loss: 0.0604 - output1_acc: 0.9376 - output2_acc: 0.9630 - output3_acc: 0.9727 - val_loss: 0.2709 - val_output1_loss: 0.1263 - val_output2_loss: 0.0879 - val_output3_loss: 0.0567 - val_output1_acc: 0.9401 - val_output2_acc: 0.9613 - val_output3_acc: 0.9755\n",
      "Epoch 99/100\n",
      "37415/37415 [==============================] - 4s 118us/step - loss: 0.2864 - output1_loss: 0.1387 - output2_loss: 0.0866 - output3_loss: 0.0611 - output1_acc: 0.9368 - output2_acc: 0.9623 - output3_acc: 0.9724 - val_loss: 0.2951 - val_output1_loss: 0.1422 - val_output2_loss: 0.0908 - val_output3_loss: 0.0621 - val_output1_acc: 0.9358 - val_output2_acc: 0.9609 - val_output3_acc: 0.9726\n",
      "Epoch 100/100\n",
      "37415/37415 [==============================] - 4s 116us/step - loss: 0.2846 - output1_loss: 0.1384 - output2_loss: 0.0854 - output3_loss: 0.0608 - output1_acc: 0.9372 - output2_acc: 0.9629 - output3_acc: 0.9726 - val_loss: 0.2788 - val_output1_loss: 0.1332 - val_output2_loss: 0.0859 - val_output3_loss: 0.0597 - val_output1_acc: 0.9370 - val_output2_acc: 0.9622 - val_output3_acc: 0.9747\n",
      "Time used: 460.8314005  s\n",
      "内存使用： 298.62890625 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Programming\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:157: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "info = psutil.virtual_memory()\n",
    "\n",
    "mem1 = psutil.Process(os.getpid()).memory_info().rss\n",
    "\n",
    "k = 0\n",
    "for k in range(1):\n",
    "    print(\"=====================================================================\")\n",
    "    k = k + 5\n",
    "    #文件导入部分\n",
    "    trainpath = 'kfold/'+ method + software + '_' + str(k) + '_20_step1_train.csv' \n",
    "    testpath = 'kfold/'+ method + software + '_' + str(k) + '_20_step1_test.csv'\n",
    "    print(\"此为第\",k,\"组交叉验证数据\")\n",
    "    print(\"……………………………………………………………………………………………\")\n",
    "    print(\"训练集为:\",trainpath)\n",
    "    print(\"测试集为:\",testpath)\n",
    "    print(\"设定训练集和测试集数据\")\n",
    "    \n",
    "    ###############数据处理部分\n",
    "\n",
    "    train_df1 = pd.read_csv(trainpath)\n",
    "    train_df1 = train_df1[cols1]\n",
    "    test_df1 = pd.read_csv(testpath)\n",
    "    test_df1= test_df1[cols1]\n",
    "    \n",
    "    train_df2 = pd.read_csv(trainpath)\n",
    "    train_df2 = train_df2[cols2]\n",
    "    test_df2 = pd.read_csv(testpath)\n",
    "    test_df2= test_df2[cols2]\n",
    "    \n",
    "    train_df3 = pd.read_csv(trainpath)\n",
    "    train_df3 = train_df3[cols3]\n",
    "    test_df3 = pd.read_csv(testpath)\n",
    "    test_df3= test_df3[cols3]\n",
    "    print(\"……………………………………………………………………………………………\")\n",
    "    print(len(train_df1),len(test_df1))\n",
    "    print(len(train_df2),len(test_df2))\n",
    "    print(len(train_df3),len(test_df3))\n",
    "    print(\"……………………………………………………………………………………………\")\n",
    "    \n",
    "    train_Features1,train_Label1,test_Features1,test_Label1 = PreprocessData1(train_df1,test_df1)\n",
    "    train_Features2,train_Label2,test_Features2,test_Label2 = PreprocessData2(train_df2,test_df2)\n",
    "    train_Features3,train_Label3,test_Features3,test_Label3 = PreprocessData3(train_df3,test_df3)\n",
    "    \n",
    "    # RNN需使用以下代码\n",
    "    train_Features = train_Features1\n",
    "    test_Features = test_Features1\n",
    "    train_Label = train_Label1\n",
    "    test_Label = test_Label1 \n",
    "    \n",
    "    train_Features1 = train_Features1.reshape(train_Features1.shape[0], 1, train_Features1.shape[1])\n",
    "    test_Features1 = test_Features1.reshape(test_Features1.shape[0], 1, test_Features1.shape[1])\n",
    "    train_Features2 = train_Features2.reshape(train_Features2.shape[0], 1, train_Features2.shape[1])\n",
    "    test_Features2 = test_Features2.reshape(test_Features2.shape[0], 1, test_Features2.shape[1])\n",
    "    train_Features3 = train_Features3.reshape(train_Features3.shape[0], 1, train_Features3.shape[1])\n",
    "    test_Features3 = test_Features3.reshape(test_Features3.shape[0], 1, test_Features3.shape[1])\n",
    "    \n",
    "    train_Label1.astype(np.float64)\n",
    "    test_Label1.astype(np.float64)\n",
    "    train_Label2.astype(np.float64)\n",
    "    test_Label2.astype(np.float64)\n",
    "    train_Label3.astype(np.float64)\n",
    "    test_Label3.astype(np.float64)\n",
    "    \n",
    "    ###############网络部分\n",
    "    #  179 = 11 + 20*2*3 + 8*2*3 \n",
    "    Features = 179\n",
    "    \n",
    "    #公共输入层\n",
    "    inputs = Input(shape=(1, Features),name='inputs')     \n",
    "    \n",
    "    #公共编码层   \n",
    "    #编码\n",
    "#     encode1 = Conv1D(filters = 256, kernel_size = 3, input_shape = (1, 179),padding = 'same', activation = 'relu',name='encode1')(attention)\n",
    "#     encode2 = GRU(64,activation='relu',name='encode2')(encode1)\n",
    "#     encode3 = Dropout(0.1,name='encode3')(encode2)\n",
    "#     encode4 = RepeatVector(10,name='encode4')(encode3)\n",
    "#     encode5 = Dense(32,name='encode5')(encode4)\n",
    "#     encode6 = Dropout(0.1,name='encode6')(encode5)\n",
    "\n",
    "#     #3个解码过程\n",
    "#     #解码1\n",
    "#     decode11 = GRU(64,activation='relu',name='decode11')(encode6)\n",
    "#     decode12 = Dense(32,name='decode12')(decode11)\n",
    "#     #解码2\n",
    "#     decode21 = GRU(64,activation='relu',name='decode21')(encode6)\n",
    "#     decode22 = Dense(32,name='decode22')(decode21)\n",
    "#     #解码3\n",
    "#     decode31 = GRU(64,activation='relu',name='decode31')(encode6)\n",
    "#     decode32 = Dense(32,name='decode32')(decode31)\n",
    "    \n",
    "    #3个编码层\n",
    "    #编码1\n",
    "    encode11 = Conv1D(filters = 256, kernel_size = 3, input_shape = (1, Features),padding = 'same', activation = 'relu',name='encode11')(inputs)\n",
    "    encode12 = GRU(64,activation='relu',name='encode12')(encode11)\n",
    "    encode13 = Dropout(0.1,name='encode13')(encode12)\n",
    "    encode14 = RepeatVector(1,name='encode14')(encode13)\n",
    "    encode15 = Dense(32,name='encode15')(encode14)\n",
    "    encode16 = Dropout(0.1,name='encode16')(encode15)\n",
    "\n",
    "    #编码2\n",
    "    encode21 = Conv1D(filters = 256, kernel_size = 3, input_shape = (1, Features),padding = 'same', activation = 'relu',name='encode21')(inputs)\n",
    "    encode22 = GRU(64,activation='relu',name='encode22')(encode21)\n",
    "    encode23 = Dropout(0.1,name='encode23')(encode22)\n",
    "    encode24 = RepeatVector(1,name='encode24')(encode23)\n",
    "    encode25 = Dense(32,name='encode25')(encode24)\n",
    "    encode26 = Dropout(0.1,name='encode26')(encode25)\n",
    "    \n",
    "    #编码1\n",
    "    encode31 = Conv1D(filters = 256, kernel_size = 3, input_shape = (1, Features),padding = 'same', activation = 'relu',name='encode31')(inputs)\n",
    "    encode32 = GRU(64,activation='relu',name='encode32')(encode31)\n",
    "    encode33 = Dropout(0.1,name='encode33')(encode32)\n",
    "    encode34 = RepeatVector(1,name='encode34')(encode33)\n",
    "    encode35 = Dense(32,name='encode35')(encode34)\n",
    "    encode36 = Dropout(0.1,name='encode36')(encode35)\n",
    "    \n",
    "    #公共编码\n",
    "    encode1 = Dense(32,name='encode1')(encode16)\n",
    "    encode2 = Dense(32,name='encode2')(encode26)\n",
    "    encode3 = Dense(32,name='encode3')(encode36)\n",
    "    \n",
    "    encodesum = keras.layers.concatenate([encode1,encode2,encode3])\n",
    "    \n",
    "    #Attention层\n",
    "    attention = attention_s_block(encodesum)\n",
    "    attention1 = Dense(96,name='attention1')(attention)\n",
    "    #3个解码过程\n",
    "    #解码1\n",
    "    decode11 = GRU(64,activation='relu',name='decode11')(attention1)\n",
    "    decode12 = Dense(32,name='decode12')(decode11)\n",
    "    #解码2\n",
    "    decode21 = GRU(64,activation='relu',name='decode21')(attention1)\n",
    "    decode22 = Dense(32,name='decode22')(decode21)\n",
    "    #解码3\n",
    "    decode31 = GRU(64,activation='relu',name='decode31')(attention1)\n",
    "    decode32 = Dense(32,name='decode32')(decode31)\n",
    "    \n",
    "    #3个输出\n",
    "    output1 = Dense(units=1, kernel_initializer='uniform', activation='sigmoid',name='output1')(decode12)\n",
    "    output2 = Dense(units=8, kernel_initializer='uniform', activation='sigmoid',name='output2')(decode22)\n",
    "    output3 = Dense(units=20, kernel_initializer='uniform', activation='sigmoid',name='output3')(decode32)\n",
    "    \n",
    "    #组合网络\n",
    "    model = Model(inputs=inputs,outputs=[output1, output2, output3])\n",
    "    # 打印网络结构\n",
    "    model.summary()\n",
    "    \n",
    "    #模型编译\n",
    "    model.compile(optimizer='adam',loss={'output1': 'binary_crossentropy','output2': 'binary_crossentropy','output3': 'binary_crossentropy'},loss_weights={'output1':1,'output2':1,'output3':1},metrics=[\"accuracy\"])\n",
    "    \n",
    "#     train_history = model.fit(train_Features, train_Label, validation_split = 0.1,epochs=200, batch_size=60, verbose=0)\n",
    "     #模型训练\n",
    "    train_history = model.fit(train_Features1,{'output1': train_Label1,'output2': train_Label2,'output3': train_Label3},epochs=100, batch_size=60, validation_split=0.1, verbose=1)\n",
    "    \n",
    "\n",
    "\n",
    "elapsed = (time.clock() - start)\n",
    "print(\"Time used:\",elapsed,\" s\")\n",
    "\n",
    "mem2 = psutil.Process(os.getpid()).memory_info().rss\n",
    "\n",
    "print ('内存使用：',(mem2-mem1)/1024/1024,'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABYMUlEQVR4nO2dd3gV5dLAf0MSSKGFJr2j9K5iQ1BUxF5R9F47dvHqvZ9dAXvDiqjYC6KCiAUsKBYQlCrSewk1lNBCSJvvjznhnEACgeQkIZnf85zn7L777u67p+zslHdGVBXHcRzH2ZsyRT0Ax3Ecp3jiAsJxHMfJERcQjuM4To64gHAcx3FyxAWE4ziOkyMuIBzHcZwccQHhlEpEZKyIXFXQfQsaETlJRBYUxbkdR3wehHO4ICI7QlZjgd1ARmD9RlX9uPBHdeiISDfgI1Wtu1f7L4H2tw7iWP2Bpqp6ZQEO0SnlRBb1ABwnr6hq+axlEVkOXK+q4/buJyKRqppemGM73PHPzMkJNzE5hz0i0k1EEkTkHhFZB7wrIvEi8o2IJIrIlsBy3ZB9fhGR6wPLV4vIBBF5LtB3mYiceYh9G4nIbyKyXUTGichgEfkov9cWsn6PiKwOHH+BiJwqIj2B+4HeIrJDRP4O9K0tIl+JyGYRWSwiN4Qcp7+IjBCRj0RkG3CviCSLSNWQPh0Dn1/UoY7fObxxAeGUFGoCVYAGQF/st/1uYL0+sAt4dT/7HwssAKoBzwBvi4gcQt9hwF9AVaA/8K9DvqK9EJGjgNuAo1W1AnAGsFxVvwOeAD5V1fKq2i6wy3AgAagNXAw8ISKnhBzyPGAEUBl4HvgFuDRk+7+A4aqaVlDX4BxeuIBwSgqZwCOqultVd6nqJlUdqarJqrodeBw4eT/7r1DVoaqaAbwP1AKOOJi+IlIfOBp4WFVTVXUC8NUBxl1bRJJCX8CJufTNAMoBLUUkSlWXq+qSnDqKSD3gBOAeVU1R1ZnAW8C/Q7pNUtUvVTVTVXcFruXKwP4RwOXAhwcYv1OCcQHhlBQSVTUla0VEYkXkDRFZETCh/AZUDtz4cmJd1oKqJgcWyx9k39rA5pA2gFUHGPcaVa0c+gIm5NRRVRcDd2KayQYRGS4itXM5btZYtoe0rQDq7GdsozHh0wg4Ddiqqn8dYPxOCcYFhFNS2Dsc727gKOBYVa0IdA2052Y2KgjWAlVEJDakrV5BnkBVh6nqiZjpTIGnszbt1XVNYCwVQtrqA6tDD7fXsVOAzzAt4l+49lDqcQHhlFQqYH6HJBGpAjwS7hOq6gpgKtBfRMqKyHHAOQV1fBE5SkROEZFyQAp2fZmBzeuBhiJSJjCWVcAfwJMiEi0ibYHrgAM5zD8ArgbOxQVEqccFhFNSeRGIATYCk4HvCum8VwDHAZuAx4BPsfkaBUE54CnsmtYBNYD7Ats+D7xvEpHpgeXLgYaYNjEK89HsExYciqpOxITO9IDAc0oxPlHOccKIiHwKzFfVsGswBYWI/AwMO5iJek7JxDUIxylARORoEWkiImUC8xPOA74s4mHlGRE5GuiIaT5OKcdnUjtOwVIT+AKbB5EA3KyqM4p2SHlDRN4Hzgf67RX95JRS3MTkOI7j5IibmBzHcZwcKTEmpmrVqmnDhg2LehiO4ziHFdOmTduoqtVz2lZiBETDhg2ZOnVqUQ/DcRznsEJEcg1ndhOT4ziOkyMuIBzHcZwccQHhOI7j5EiJ8UE4jnN4kpaWRkJCAikpKQfu7Bwy0dHR1K1bl6iovNd/cgHhOE6RkpCQQIUKFWjYsCG512hy8oOqsmnTJhISEmjUqFGe93MTk+M4RUpKSgpVq1Z14RBGRISqVasetJbmAsJxnCLHhUP4OZTPuNQLiG3b4JFHwH+fjuM42Sn1AiItDQYOhJdeKuqROI5TFCQlJfHaa68d9H69evUiKSlpv30eeugh2rZtS/v27Tn99NNZs2bNIY6yaAirgBCRniKyQEQWi8i9OWy/SUT+EZGZIjJBRFoG2huKyK5A+0wReT1cYywfqDq8Y0e4zuA4TnEmNwGRnp6+3/3GjBlD5cqV99vnf//7H7NmzWLmzJmcffbZDBw4MD9DLXTCFsUUKA4/GCt+ngBMEZGvVHVuSLdhqvp6oP+5wCCgZ2DbElVtH67xZVG2LEREwM6d4T6T4zjFkXvvvZclS5bQvn17oqKiiI6OJj4+nvnz57Nw4ULOP/98Vq1aRUpKCv369aNv375AML3Pjh07OPPMMznxxBP5448/qFOnDqNHjyYmJoaKFSvuOc/OnTsPO19LOMNcjwEWq+pSABEZjhVP2SMgVHVbSP849i28HnZETItwDcJxip4774SZM/PW99df4eSTD9yvfXt48cXctz/11FPMnj2bmTNn8ssvv3DWWWcxe/bsPeGg77zzDlWqVGHXrl0cffTRXHTRRVStWjXbMRYtWsQnn3zC0KFDufTSSxk5ciRXXnklAA888AAffPABlSpVYvz48Xm7uGJCOE1MdYBVIesJgbZsiMitIrIEeAa4I2RTIxGZISK/ishJOZ1ARPqKyFQRmZqYmHjIA3UB4TiHH3kRDofCMccck22uwMsvv0y7du3o0qULq1atYtGiRfvs06hRI9q3bw9Ap06dWL58+Z5tjz/+OKtWreKKK67g1VdfDc+gw0SRT5RT1cHAYBHpAzwIXAWsBeqr6iYR6QR8KSKt9tI4UNU3gTcBOnfufMjahwsIxyke7O9Jv7CIi4vbs/zLL78wbtw4Jk2aRGxsLN26dctxLkG5cuX2LEdERLBr1659+lxxxRX06tWLAQMGhGfgYSCcGsRqoF7Iet1AW24Mx8odoqq7VXVTYHkasAQ4MjzDdAHhOKWZChUqsH17zhVWt27dSnx8PLGxscyfP5/Jkycf1LFDtY3Ro0fTvHnzfI21sAmnBjEFaCYijTDBcBnQJ7SDiDRT1axP8CxgUaC9OrBZVTNEpDHQDFgaroHGxbmT2nFKK1WrVuWEE06gdevWxMTEcMQRR+zZ1rNnT15//XVatGjBUUcdRZcuXQ7q2Pfeey8LFiygTJkyNGjQgNdfD1tAZlgIa01qEekFvAhEAO+o6uMiMhCYqqpfichLQA8gDdgC3Kaqc0TkImBgoD0TeERVv97fuTp37qyHWjDorLNg/XrwekOOU/jMmzePFi1aFPUwSgU5fdYiMk1VO+fUP6w+CFUdA4zZq+3hkOV+uew3EhgZzrGFUr48LFlSWGdzHMc5PCj1M6nBfRCO4zg54QIC90E4juPkhAsIghpEGN0xjuM4hx0uIDABkZ4OqalFPRLHcZzigwsIPGGf4zhOTriAwAWE4zh5p3zghrFmzRouvvjiHPt069aNA4Xdv/jiiyQnJ+9Zz0v68NwIV1pxFxCYkxrcUe04Tt6pXbs2I0aMOOT99xYQeUkfnhvhSivuAgLXIBynNHPvvfcyePDgPev9+/fnscce49RTT6Vjx460adOG0aNH77Pf8uXLad26NQC7du3isssuo0WLFlxwwQXZcjHdfPPNdO7cmVatWvHII48AlgBwzZo1dO/ene7duwOWPnzjxo0ADBo0iNatW9O6dWteDCSoWr58OS1atOCGG26gVatWnH766XvOE6604kWerK844ALCcYoJRZDvu3fv3tx5553ceuutAHz22Wd8//333HHHHVSsWJGNGzfSpUsXzj333FxvvEOGDCE2NpZ58+Yxa9YsOnbsuGfb448/TpUqVcjIyODUU09l1qxZ3HHHHQwaNIjx48dTrVq1bMeaNm0a7777Ln/++SeqyrHHHsvJJ59MfHx8oacVdw0CFxCOc1hSQPm+O3TowIYNG1izZg1///038fHx1KxZk/vvv5+2bdvSo0cPVq9ezfr163M9xm+//bbnRt22bVvatm27Z9tnn31Gx44d6dChA3PmzGHu3Lm5HQaACRMmcMEFFxAXF0f58uW58MIL+f3334HCTyvuGgQuIByn2FBE+b4vueQSRowYwbp16+jduzcff/wxiYmJTJs2jaioKBo2bJhjmu8DsWzZMp577jmmTJlCfHw8V1999SEdJ4vCTivuGgTupHac0k7v3r0ZPnw4I0aM4JJLLmHr1q3UqFGDqKgoxo8fz4oVK/a7f9euXRk2bBgAs2fPZtasWQBs27aNuLg4KlWqxPr16xk7duyefXJLM37SSSfx5ZdfkpyczM6dOxk1ahQnnZRjzbQ9hCutuGsQuAbhOKWdVq1asX37durUqUOtWrW44oorOOecc2jTpg2dO3c+4A335ptv5pprrqFFixa0aNGCTp06AdCuXTs6dOhA8+bNqVevHieccMKeffr27UvPnj2pXbt2Np9Bx44dufrqqznmmGMAuP766+nQoUM2c9LehCuteFjTfRcm+Un3nZYGZcvCwIHw0EMFPDDHcfaLp/suPA423bebmICoKChXzjUIx3GcUFxABPCMro7jONlxARHAa0I4TtFRUkzdxZlD+YxdQARwAeE4RUN0dDSbNm1yIRFGVJVNmzYRHR19UPt5FFMAFxCOUzTUrVuXhIQEEhMTi3ooJZro6Gjq1q17UPu4gAjgAsJxioaoqCgaNWpU1MNwcsBNTAHcSe04jpMdFxABXINwHMfJjguIAC4gHMdxsuMCIoALCMdxnOyEVUCISE8RWSAii0Xk3hy23yQi/4jITBGZICItQ7bdF9hvgYicEc5xggmInTvBI+0cx3GMsAkIEYkABgNnAi2By0MFQIBhqtpGVdsDzwCDAvu2BC4DWgE9gdcCxwsbcXEmHHLInus4jlMqCacGcQywWFWXqmoqMBw4L7SDqm4LWY0Dsp7fzwOGq+puVV0GLA4cL2x4RlfHcZzshFNA1AFWhawnBNqyISK3isgSTIO44yD37SsiU0Vkan4n2biAcBzHyU6RO6lVdbCqNgHuAR48yH3fVNXOqtq5evXq+RqHCwjHcZzshFNArAbqhazXDbTlxnDg/EPcN994VTnHcZzshFNATAGaiUgjESmLOZ2/Cu0gIs1CVs8CsurmfQVcJiLlRKQR0Az4K4xjdQ3CcRxnL8KWi0lV00XkNuB7IAJ4R1XniMhAYKqqfgXcJiI9gDRgC3BVYN85IvIZMBdIB25V1YxwjRVcQDiO4+xNWJP1qeoYYMxebQ+HLPfbz76PA4+Hb3TZcQHhOI6TnSJ3UhcXXEA4juNkxwVEAHdSO47jZMcFRIDYWHt3DcJxHMdwAREgIsKEhAsIx3EcwwVECJ7R1XEcJ4gLiBCyMro6juM4LiCyERfnGoTjOE4WLiBCcBOT4zhOEBcQIbiAcBzHCeICIgQXEI7jOEFcQIQQF+dOasdxnCxcQITgGoTjOE4QFxAhuIBwHMcJ4gIihPLlYdcuyAhrYnHHcZzDAxcQIWRldHU/hOM4jguIbHhGV8dxnCAuIELwmhCO4zhBXECE4ALCcRwniAuIEFxAOI7jBHEBEYI7qR3HcYK4gAghy0ntGoTjOI4LiGy4iclxHCeIC4gQXEA4juMEcQERQpaAuP32oh2H4zhOcSCsAkJEeorIAhFZLCL35rD9LhGZKyKzROQnEWkQsi1DRGYGXl+Fc5xZREdDs2bQvj2oFsYZHcdxii9hExAiEgEMBs4EWgKXi0jLvbrNADqraltgBPBMyLZdqto+8Do3XOPMPmZ48EGYORO+KhSR5DiOU3wJpwZxDLBYVZeqaiowHDgvtIOqjlfV5MDqZKBuGMeTJ/r0gaZNoX9/1yIcxyndhFNA1AFWhawnBNpy4zpgbMh6tIhMFZHJInJ+TjuISN9An6mJiYn5HjBAZKRrEY7jOFBMnNQiciXQGXg2pLmBqnYG+gAvikiTvfdT1TdVtbOqdq5evXqBjeeKK6BJE9ciHMcp3YRTQKwG6oWs1w20ZUNEegAPAOeq6u6sdlVdHXhfCvwCdAjjWBEJLkdGwkMPmRZRpliIUMdxnMInnLe/KUAzEWkkImWBy4BsRhsR6QC8gQmHDSHt8SJSLrBcDTgBmBuOQarCrbfCffdlb7/iCugQEEk33wzbt4fj7I7jOMWXsAkIVU0HbgO+B+YBn6nqHBEZKCJZUUnPAuWBz/cKZ20BTBWRv4HxwFOqGhYBIQKpqfYKJTISJk6Eu+6CN96ANm2yaxmO4zglHdESYmTv3LmzTp06NSzH/uMPuPxyiImBOXMgIiIsp3Ecxyl0RGRawN+7D25hD2HxYsjM3Lf9+ONh0CBYsAA++6zwx+U4jlMUuIAIMH68zaL+7ruct19wAbRqBY8+ChkZhTs2x3GcosAFRIATT4Qnn4ROnXLeXqaMRTbNmwcjRxbu2BzHcYoC90HkgEjO8x8yMsxZXaYMzJrlIbCO4xz+5MsHISJNQkJOu4nIHSJSuYDHWGyYPBlefTXnbRERNss6y1G9dm3OPgvHcZySQF6egUcCGSLSFHgTm/w2LKyjKkI++ggefxx27cp5e+/e5osAqF0bYmNN4/B5Eo7jlDTyIiAyA3MaLgBeUdX/AbXCO6yiY+BAWLTIQlpzIiICJkyAb7+FwYPhhhus7eSTYd26wh2r4zhOOInMQ580EbkcuAo4J9AWFb4hFS1Vqti7KqSlQdmy+/apXBl69Qqun3kmXHIJHHecRUEddVShDNVxHCes5EWDuAY4DnhcVZeJSCPgw/AOq2hJS4OuXS1qKS/06gW//AI7d0Lz5rBwYViH5ziOUygcUECo6lxVvUNVPxGReKCCqj5dCGMrMqKioEsXu9nnlaOPht9/h+rVoUcPWLXqwPs4juMUZ/ISxfSLiFQUkSrAdGCoiAwK/9CKlmefhWuuObj8S0cdBd9/D1u3wmmnQQGVqHAcxykS8mJiqqSq24ALgQ9U9VigR3iHVTxIT4fRo/dN5Lc/OnSAb76BFSugRg07huM4zuFIXgREpIjUAi4FvgnzeIoVP/0E55138JXlTjoJ3nvPll9/vcCH5TiOUyjkRUAMxFJ2L1HVKSLSGFgU3mEVD047Db7+Gs4//+D3vfRS2/+hh2DDhgP3dxzHKW54qo0wMn++pea46ip4662iHo3jOM6+5DfVRl0RGSUiGwKvkSJSt+CHWXx56y2bXX2wNG8Od94Jb78Nf/1V4MNyHMcJK3kxMb2LlQqtHXh9HWgrNUyeDOPG5ZzA70A89BDUqgXHHgtffAHJyQU/PsdxnHCQFwFRXVXfVdX0wOs9oHqYx1WsePVVqxdRpgzcdhv88EPew18rVoShQ6FqVbjoIpsnIQLLl4d1yI7jOPkmLwJik4hcKSIRgdeVwKZwD6w4ER1t77t2wfDhls01KxVHUlLO+/zf/5kgyMyEs86yPE3jxpk/onx5S/j3/PMeBus4TvElLwLiWizEdR2wFrgYuDqMYyq2REfDxo3wn//Y+qOPmhM6J22iUiXL/JqWZuuRkXDqqfDaazB3ri3/9782a7tNG6hZ047z22+Fdz2O4zj745CimETkOVX9bxjGc8gURRTTlCkWBjtwoK1fdRV88EHefBWq5pN44w3TKKpVg59/hh07YPp0SyXuOI4TbvYXxXSoAmKlqtbP98gKkOIQ5nrNNZYmfPBg0wbS0uDXX01byNIy/vrLHNY5fexz5sAxx9hs7PHjTbtwHMcJJ/kKc83tmPkYT4nl3XfNhJQlDD75xCbLZYW4ZmbCZZfBKafkvH+rVhZSO3GimZ/mz4cxYyxM9mByQjmO4xQEuWoQgeR8OW4C/lbVYjUXojhoEHuTlGQZXk8/HcqVs7a5cy2ZX3S0aQo5cccd8Mor2dsiI+GKK+Cee6BFi7AO23GcUsShahDTcnlNBfKUvk5EeorIAhFZLCL35rD9LhGZKyKzROQnEWkQsu0qEVkUeF2Vl/MVNypXhnPOCQoHgJYt4fPP4YQTcp8T8dxzpo189JFpE7NnW3jtZ5+ZliECDz9s2zIy8j6eUaNcE3EcJ+/sT4Moq6oHkcd0n/0jgIXAaUACMAW4XFXnhvTpDvypqskicjPQTVV7B7SXqUBnQDHB1ElVt+R2vuKoQQBs22aJ+5KSYP16ePppy/S6dCmccUbOFetyIzER3nzTyp3++aeZrMBSk193HcTH577vrFlWsyI1FYYNg8svz89VOY5TUjgkJ7WITMVu7N8B36nq8oM86XFAf1U9I7B+H4CqPplL/w7Aq6p6QqDEaTdVvTGw7Q3gF1X9JLfzFVcBsWULHHEExMVBw4YwbZpNuCuI437/vWWL/fVXiI01jWT7douKCiUlxYRDYqKNYc4cmDrVS6M6jnOIJqbADncGVl8UkSki8oKInC4i5XLbL4Q6QGhdtYRAW25cB4w9mH1FpK+ITBWRqYnFtDpPfLxpC0lJFhabJRzWrYMhQw59olx8vDm8f/kFZs60Wdpg/omRI7NHSd17r5mp3nvPzFvlylm22V27Dv26HMcp+ez3WVZVl6vq66p6PnA8loepB/C7iHxbUIMIzM7uDDx7MPup6puq2llVO1evXnyzf9StazfsyMhg28SJcMst2ZP4icCkSQef86ldO5t/MXGipfS4+GITRH36wP/+By+9BP36Qc+eUK8efPihmZxiYw/Oh+E4TukiL9lc40SkjKqmqerPwH1Ad6DvAXZdDdQLWa8baNv7+D2AB4BzVXX3wex7OHPmmRbGevzxwbbbbrMQ2G3bbD0nh/LWrbkLkOOPN9PRa6/BuefChAnm8G7bFp56Kvu5H3nEls86CzZvLphrchynZHHAiXIiMhnooao7AuvlgR9U9fgD7BeJOalPxW7uU4A+qjonpE8HYATQU1UXhbRXwRzTHQNN0zEnda63suLqg9gfqvDEE5bQ7/bbzX8wdSp0727bVq827SOL5cuhcWPbllctY/NmC6mNjd1329ChcOutplV88AE0aGBO89jYff0YjuOUTPI7US46SzgABJZzuN1kR1XTgduwanTzgM9UdY6IDBSRcwPdngXKA5+LyEwR+Sqw72bgUUyoTAEG7k84HK5s3AgPPmiOa1WoUMGEA8Bdd0GnTtlDYTMzoUsXWLUq5+PlRJUqOQsHgBtuMAf3rl1w4okmKI44wsZRsaKF5IrY/Is777TlNWsO+XIdxznMyIsGMRG4XVWnB9Y7YdFGxxXC+PLM4apBzJtnjuW9zUmTJ8Pff1v6jpxCYRcuhKZNCyYiasMGi4javdvCYLdvN0GQkGDviYnWZ/t2G8u//mUzvZs3z/+5HccpWvKVi0lEjgaGA2uwWdQ1gd6qOq2gB5ofDkcBcTAMG2b+iZo1LZnfscdaiOt11xXeGJYuhUGDLPVHSoq1jRlj8zkKQlA5jlP45MvEpKpTgObAzcBNQIviJhzyRUoK/PRTsZxirGphqYMHmybx3HPW3qEDPPYYnHde4Y6ncWMrnrRyJfTvb8KqVy+bTyGyr+mrhJQ7d5xSS16imG4F4lR1tqrOBsqLyC3hH1ohsW0b9OgBL79c1CPZBxG7IY8aZZPb7rkn2H7PPZYiXMSe4GfPLjwZV726RUGtWGGaTc2a1t6gAXTrZuNo3NjMUSJw9dWecNBxDkfyYhi4QVWTslYC6S5uCNuICptq1Syv9uriGUX72WfmH2jWzG7MezN+vD3RV6pkT+yJieZHKAzKlrWUHb//DosXm9DYssXMX126mKP9ggssNcj119s8kEsvtboXrl04TvEnLz6If4C2GugYyLE0S1VbFcL48ky+fBANG8JJJ9kMssOcXr1g0yabgJeZCe3b26S4rK9ZpPBvzqqmAb33niUhzJp38d13lunWNQvHKTryG+b6HfCpiJwqIqcCnxBMiVEyqFOn2GoQB8utt9pL1W68J58MI0bYtuRkC3kdNy7vxxMxoZN1vENBBFq3Nh/K6tUmJBo2tJndPXoUjdByHOfA5EVA3AP8jDmobwL+AWLCOahCp04di+ksAZx1Fvz738H1l18O5mlKTbWb8pFH5r5/RobdwHcEZr689JLlfcq6id90U3AW9qEQHW0+ifnz7dizZln7kUfC449bVNTrr8N999k5N2069HM5jpM/Ig/UQVUzReRPoAlwKVANGBnugRUqdeqYoTw/j8mHAZUrWyI/sEstU2bfJ/fffoNrr7VyqX37WvGiLFSDcyUgf0/+5crZsa+7zjScd9+1SYNZZOWtatDAxnHXXdlnlTuOE35y1SBE5EgReURE5gOvACsBVLW7qr5aWAMsFOrUMfvL1q1FPZJC4403gskBQ+ne3Sbo3ZBDGIKI3cifeMLWly499Gy0WcTFwVVXWVbapUstf9SqVRZ9PHs2XHihaUFZSQYdxyk89mdimg+cApytqieq6itAycz9mfVoWkL8EHnhyivNxNOli6XaELE8UGDJ/fanSInAggU2A/y113Lvt2PHwWkYjRpZpb26dSEiwqrnffABLFoEXbua6eyaa2DnzuA+7rtwnPCxPwFxIbAWGC8iQwMO6pJpf6kTKDVRigRE+fKWIFDEntYbNLBw2bxy5JEwcKClFs+JefMsMKwgZlg3amRzGR96CN5/38ZetaqF2ZYpY7msHMcpeHL1Qajql8CXIhIHnIcVD6ohIkOAUar6Q6GMsDAohQIilHLlzMTTsGHe9xGB//u/3LfPn29zItauze/ojMhIE0jdulmt7pgYSyr46afmeP/9d88N5TgFTV5SbexU1WGqeg5Wl2EGFtlUcqhd295LqYCIjT044RBKYqJNftvbJHXBBZZQsGpV+GE/jxKZmQdnJjrlFHjnHUs/8tRT8OOPZo46/XTTgGbPhv/8x8bzxhvBut2O4xw8B2UAUNUtgSpup4ZrQEVCdLTdyUpIqGthEhtr5VQ/+sjWv/rKQlXBTEAvvGCpQObN23ff9HS74efHDNW0qc0037bNzGRt2pjwaNnSQnKPPx5mzDj04ztOacZzcGZRgibLFSZxcXaD7tPH1v/4w+ZiZD2533gjfPNNzuafXbvM4fzNN/kbQ7t2FqV8yimWbXbNGtMkPvwQli2Djh1NozjySKt7IWLhuo7j7J8DzoMoNbiAOGRCzUvt2pkilqUVVKpkAiOrX6g5qUIFmDLFltPSTGBUrHhoYzjhBHNkh3LllXbuoUMtdDYx0cxeYLmtHnzQ+uxdUCkhweaMeFU9p7TjGkQWdeu6gCgALr886PMPZdo0y4n46ac20e6OO4Ifd3o6HHcc3BKGHMHx8eZMf+UVGD7cxvHDDzbGG280Dej00+HZZ02AHXWUzbmoUMEm5y1fXvBjcpzDBRcQWdSpY2XTCisVainjyCPh7rvNHzF9ut2wp0+3bZGRNsfhkkvCPw4ROO00M4WNH29CYPVqEyLR0Zam/PnnzWT2yivQpIntM2GCz7lwSh8HzOZ6uJDvinJvvWXTh5cvN2+nE1Y2bIAaNfZtFzGfRvfupnHkl5QUO2Z09P5v8OvWmTksJiTLWEKCObzffNMy0HbubJMJ77nHTGKDBsGTT5qWctNNlsfKK+s5hxv5zeZaOijlcyEKm5yEA8DcuTavYdAgu7GnpeUvVPWNN0zer19vwuLTT+1Gvjc1a2YXDmBWxyefNP/FkCHBBIYvvGDHLVvWkgredJO1H3mkaR+eYNApKbiAyCJLQHioa5HSvDmMHm3+CFW7oZcvH3SEDx1qy9u35+14HTua+apGDUsKeNllwUSAeSU21oTAvHkmrHbvNmGxe7el8EpIgE8+sek0//2v1aC67rqgCW3TJouyKi7RU/nNn+WUHkq9gFi7fS3Vn62OjGpnDa5BFCkicM455iQGcxrffHPQNRQfbzf5tLS8He+kk+CZZ2y5T5/8+xL2nhAYE2PPFpddZplwZ80yYTJ8OHTqZP2rVYOzzw6OJyfHt6rNITnnnKDmVNBkZpqwjIqCjRsL/vhOyaPUC4gqMVXYmLyRgd0GWM4JFxDFiqOPNrNNlj/i4ovtab1KlQPvO3x4sHodmH/ghBNseeFCc5jvLynhyy/b62AywLdpY+ao1avhxRehd28zU/3yi5WPXbDAtJovvrCa3itWWJqQbt3gvPNg4sTgcb7+Omdz2KGgajPMP/zQrufuuwvmuE7JptQLiHKR5agSU4W1O9b5XIjDiOXLrXBRbtrAihUWcjtkSM7bV62yY/z9d/b20NKs48bZ3IpD0TgqV4Z+/UxI3XuvVfa75BILs23QwIo4NWxor65dTft47TXzlYwebec891wzhzVpAqeeamOaP//gxwLw9NMm7P7zH7j/fsuS++OPh3Ysp/QQ1igmEekJvAREAG+p6lN7be8KvAi0BS5T1REh2zKw6nUAK1X13P2dKz9RTK1fa02zqs0Y9dpm+2f+9tshHccpPIYMMXv/9OlmhsqJf/4xv0DVqjlvz8iwPE5ZbNlipqBHH7VZ2enpdoPOzLRaFU2bHvw4cyqqtGuXCYFdu2y9bFkTBllmNTAT02efWS3v5cst5fm0aXastm2Dpqy4ONOwrrvO/CKffgpvv20RV1FRFnX1++/mu+nTxzSI1FSb0JiWZjPO954o6JQu9hfFFLaZ1CISAQwGTgMSgCki8pWqzg3pthK4GvhvDofYpartwzW+UGpVqMXa7WuhTmP488/COKWTT/r2tZtq3br7hpcuWmQRRQd69omIsD7PPWc3zWuusUinrP2ynNk33WT+geXLLVw2L6SnWzr1nHJQxcSYz2J/REXBFVdkb1u92hztI0ea5jFypKUqqVABxo41YVGxogmLUOLibMb422/b5xQdbaG73brZttDPTxW++87qkP/vf3DEEXauiy8unHkgKSn2+ZSQ6PvDnnCm2jgGWKyqSwFEZDiWNnyPgFDV5YFtRZpzs2b5mizctBDqnGT/whJeerQkEBFhFkFVMzXNmmVP3IsWmf3+1YOoefjPPyYg6tTJeS7DddfZvIyDiX5asMDG062baQiNG+d939yoU8fMVv365d4nMxMWL7YUJunppkk0b55dU8rSavr2NUFRr56Zv5o1M81szhwTOi+/HIy6uvBCWy5XLv/XkRupqWZKK1/ecmg1ahS+czl5I5w+iDrAqpD1hEBbXokWkakiMllEzs+pg4j0DfSZmpiYeMgDrVW+Fut2rENr17Z/gQeyH1bEx1ukUNmyVuXuxRfNxp8XRGyO5LBhwTrde3PssebPyE1ATJ68bwhrq1Z2o16zxrSZNWsOPI6CoEwZO98VV1gp11atgsJh/fpgSVlV+5w+/hiOOQZefx1uu82u8cMPLcpp1y4zuz3/vDnVzz3XNBYRc/6PHGnLTz9tQjq/13DXXTbDPS3N5qy6FlEMUNWwvICLMb9D1vq/gFdz6fsecPFebXUC742B5UCT/Z2vU6dOeqgM+mOQ0h/dOOxtVVCdOfOQj+WUTFJTVYcOVf3553233XabaoUKqmlpqpmZquPG2buq6pIlqq+8orp9+777bd2q2qeP6sqVqikpqrNmhfcaJk1SbdxYdd481WuvVb366uA4t25V/fvv4PrevP22qohqs2b2FylTxt5DXyKqN96ounGj7bNrl+q0abbtwQdVL7nElqdO3fc8H3xg2+6+W3XIEFseOjR8n4UTBJiqud3Hc9uQ3xdwHPB9yPp9wH259N1HQBzMds2ngPjkn0+U/ujs7z+0j2TkSNXff8/69A75uE7JIS1NtUGD7D+HXbvsPSNDdcUKW16+3Pp8++2Bj/nHH6pVqqiOH29CpmJF1Q0b7Hhz56omJe3780tJMWF1qD/L9HR7f+QRex3McVq2NMHQrp3qfffZ+NetU01OtrHeeadqRIRdE9hylvCIiFBt2lS1XDlbb9PG3p96SvWll2z55JPtc87IUO3WzT6PVasO7TpVbVzOgSkqAREJLAUaAWWBv4FWufTNJgCAeKBcYLkasAhoub/z5UdA/DnufVXQn8e9lf2RKDHROsyYYb9cp1QzY4bdCFVVP/7YfiKbN2fvM3euPS1nZATb0tNVv/hCdeLEfY+5dau9JySofvSRLWf9/D77bN/+jz9uT/FJSaYJ5PbEH8qOHfaEHjqmUH76yc6V2/Ysdu1SHTtWNT5eddiwfbdnZKi++KJqly5BreHzz+0z2b3b+mzebBrCsceqRkcHr7VePRM2WSxerBoba9vmz89+nrlzrT03AbBypep11wUFVF4+o3CxapWNIScNsrhQJALCzksvYCGwBHgg0DYQODewfDTmm9gJbALmBNqPx0Jc/w68X3egc+VHQKx+7xVV0K+H9VeNibGPpUoV1fPPt+WoKNVjjrHljz9WvfRS1yxKOUOGqN5xR/DGtz/S0lTr1FG98krr3737/n8+u3ervvuuCQ3VoMlGVfW778wMM2OG/Sxfe+3A5x871s43bVrO2y+9NGg6yomMjOBNNiND9dZbzUy0N6Datq1qr14HHlMWO3ea9pXTzX7o0KAA6do1ePystjp1VN96yz7fnTtVf/lF9T//sW1lywb3ueWWfYVfcrLq9OlBQZ8lqFXtWjduVN2yJe/XkRPz55vgA9Wzzw5qbwVFQd2CikxAFOYrPwJi25b1Gncf+vSEp1UXLFB99VUzDjdqZHpuVJQZWLN+mVkG2DPPDP4K33lH9c8/bfm//8365Pc9WUaGC5dSyIIFwa/9yitVH344bzeMe+4x09beT6CZmarPPGM3MTCfwq23Bs/x9NPB5fR00yByIz3dTGO5MXKkmZVWrsx5+/vv2w1a1Y6TlmavnPrn9aefmWnX/vPPqk8+qdqkie17/PFmkvr2W9NCsv6SkZG6xw/y73+rLltmx/i//7P2q66ycV5/vWrz5sH9Qv0nWcKncuVge926qj172vLRR6tWr657tLv9aSZTp6pWq6Zao4bq//5n+9x+e96uPa9s2VIwhg0XEHkg7vE4vXPsnbl3yMy0R4K33zb9tUcP1c6dVStVyi48sn6t3brZ8kUXmfBQtX9as2bZH1ccZz/8/rvqY4+ZA/vFF3PXWO66S/X++4Prxx2net55B3eulBS7gb75Zvb2b79VPeus7AJt7VrVUaMsnkNEddCg7Pucfrpqp07Bm+hff6lu26Z6002q77134LEsWWJ/rTfeMK0pLS1o3ssiM1P100/tb3bPPapff626adO+ffr3D/41K1e2a3n4YbvJf/SRCdYBA1TPOEP1tNNM43j+efOPXHmlCccjj7Rt118f9J+ccIK9z5plwnDJEjO93X67tTdooLpwoY3jrrus7cUX973WFSuCgqxPn6Az/6GHgg77ULZvt3NWrmyaZn5xAZEHmr7cVHt/3vvQdk5Pt1/CqFH2uHP11fYPjYrSPbowmGZyyimqS5faL/6pp4KPXlkeQ/esOTmQ9fPYsCF858jIsJvkgAG2PnmyCQ3VfW9St94a9CH8/vu+Jpyvv1YdPdpu0OvWmT+hXz9zRGcd/0Bs2WK+G7D3nBg8WPdERu2PSZMsSitrnGlppu383/+ZIDqYv116upm/jjgi+3Nh1is21owLWSZCVTvvBRcEnyXfesueN/v1M3NYVJSZo5o2NQ3niCOCfaOjTVjs2GGv88/Pvq1mzX2/n4PBBUQeOOmdk7Tru13zdYx92LjRHhlat973V1S/fnD5f/8LPo7kpsc7pZrMzP2bgQqKLC0h66c5ZEjO/VauDD4dH4jMTFOes8xPB2LbtuByRoZpCbntt3WrPe0frCP6xRft5jppkt10D0RON+Dt2y2kecgQ0/LA/Bq5jXXnTtPyGjfOfiu4/vqc//ZpaWaavPzy7M+ZWbeP/v3NH3XDDXkXujlfmwuIA3Lp55dqs5eb5esYuZKZqbp6tRlUX3/d9M0LL1Rt0cKMmqHhHCKqHTsGBYbjFAGZmWYKysvNMzd27lR94QXV2bP33TZrluqYMTmf99hjTQk/WHKavgQWFrw3q1ZZRNj+BEtmpj3hv/eeRXDdf39QeGXtl+VAP/nk4H4//ZRdyOV03KlTTbDtHaGVG7//buc4++xgBL6qaZQPPZQ/w4MLiDzQb2w/Lf9E+XwdI1+kpakuWmSxgZUqBQVG9+5mtsoSFj/+6ILDOSzYtCn4Mw4lMzNox88CLCQ2Lc2W9w6jzfrZh5rY7r1X9YcfbPmjj3SPueuDD1SHDw/26dDBhFVqqpm+/v47+7knTzafSWi0mKqZ10491cxQv/5qZqCRI82f0rFj0IT088+2XdXCeLOe9w4UNnwoLFpkIcBZjB8fjJk5VPYnIMKZi+mwolb5WuxI3cGO1B2UL1u+8AcQGWnpQh99FPr3t1zMP/9sWdjuu8/6dO4cTL25dypSxylmVKliaT32Li8rYmnUszLrp6dDrVqWZiO3pICVK1sW2507oXp12LbN6oKULw+nnQYXXGCpQzp3tqJMZ54Jl15qaVJU7W8jYokA+/bNfo7ISMv9tHx59sy/0dGWHyoiwlKYLFxoiQtFbCxbtlh+rO7ds49z8mTLB5bX+uSZmZZ+/Zprcr72UJ5+2nJ8bdxoCR27dbNMww0a5O1cB01ukuNwe+VXg3hvxntKf3TRpkX5Ok6BkxWoDRYWkhVme/HF9vgUjscUxymGhP7UMzJM28jJtLJlS+7RXjNn7ms2y8zMbmr66SezAGfNlM+JvPg8du3aN6ggp2NOnRrUoFTt2rJ8EpmZti3Lr7F0ad5m6R8M7EeDKPUFg7KoVaEWYCVIixV9+lhq0J07Lbn/vHlw552mXZx+uj3ePPWUZWJznBJMmTL2pHTnnfazj4w0jWBvKle2xI050a6dpTgPRSSY4TY93TSJxYvtL5cbB0pMmJEBHTrAHXcE27ZtMyPB3vt26mTaU1aCyaefhvr1TbP4/Xdr+/BDe2/UyFKxFxYuIALUKh8QEDuKmYAAS8+ZZVpq3tzSay5bZrp2fLyZoOrVs1/e66/Dhg22/MknpseL2K/944/3TTvqOIcRIvaTP+YYSE4uuOOuW2dmmqgouPZaS5eeW6GpvBARYfU0brwx2FaxItx6K7z/vq1//72Z2gCOOy4oOCpVMoGQmWnVBn/4Aa6+2up0jB176GM6JHJTLQ63V35NTIk7E5X+6IuTXszXcQqVLVtM55w3T/Xmm4OmqNCJe1deqXuCs0Nj5N58M+fwjoLGHepOGMian1FQZGZa4GCW07sgj/vAAxbNtTf33qsH5cy+557w/J1wE9OBqRpTlagyUcVTg8iNypVN52zeHK6/Hh57zJL09+8PPXtavc2PPrLqL82aBZ3aNWuap65sWXjllQPr0nPmHNr4VE3TcZwCpqALF4nAt99a7fCCZNcuePxxmDt3Xwd0w4ZWJySvzux27axOemES1prUhUl+alJnUf+F+nRv1J33z3+/gEZVDNi+3fTmiAgrp3riifZL7dvXKuWA6dKbNsF775mRtEkTqzUpYuEgs2fD+PEmZDIz81YZZtEieOMNK1m2bp0XPnZKJZmZ8NdfFk1VXItU7q8mtWsQIeypTV2SqFDB4vWiokw4gFW3//preOABmDDBDKBghs4TT7SYw3LlrDzbd99ZPF+zZtbn9NOtnNj+SE2144wbBy+8UHz/GY4TZsqUgS5dDt+/gM+DCKFW+Vos2bKkqIcRfipUsCf8rVuhbl0LrL74YitMfMQRFiYRF2emq1q17L1dOxg1yjxrbdqYKatLl5yPX6aMFYWuVy/3Po7jFHtcQIRQq3wtJq6aWNTDKBwqVLAXWKzgt98Gt+3YYVpD3brZ9znrLHjiCRgzxjSEjAwYNAjOPjuoYYDFH15yiS0nJ8Po0VaNfu8ZU47jFGvcxBRCrQq12Ji8kdSM1KIeStESF7evcABre+01mDkzGL93110WhitiVe/fftvi+77/Hn79FaZPt7kcX3xRqJfgOE7+cQ0ihKy5EOt3rKdepXpFPJpiTLly8MsvQbPTN99YqMabb0JamvV57rlg/6OOghUrgrORHMc5LHABEcKe2dQ71rqA2B8xMRYRlZFhs3quu84Exkkn2bTQrl3NQZ0VwvH990GBceyxcPPNcOGFNnPIcZxii5uYgPkb5yMDJDibuqRFMoWD8uVNOICFsI4cCf36mZBQNZ/DaadZpNRvv9k7QFKSZSWrVMlCai+91DSLiy82/4YIPPII3HuvLb/0ks0IF7Hj/PWXZSdzTcRxwk6pnwehqnR4owNREVGMvmw0dQbVsfZHSsbnUmx45RXTKkaNgokTLXJq9WrzZ6xaZUlwQomIMA0lN+rUMSH0wQe2b2hm2/R0C+tVhUmTLGLrqqtcqDhODuxvHkSpNzGJCO+d/x61K9SmSkwVYqNiOfvIs4t6WCWP2283J7aIZSJ79VVzYnftahPpduwIZijr18+in95/36Kphg2Dxo3NtLV5s0VItW8fjLw65hhznh97rE3Oy0qPPmGCOcdHj7b8VI7jHBSlXoPYm4fHP8yjvz3KH9f+wXH1jiuAkTn78NNP8M8/5rvICrXdG1UTJps22dyMjAxrS0qybG2vvmq+jIgI0xaynOPlylkywpgYSEmBG24w81aW8Fm92uZnOI4D+EzqPLFm+xrOH34+J9Q/gVrla3HXD3dRUoRnsePUUy1nc27CAYJTT6tWNTPUjBm2XqmSCZdLL7UJecuWmRZy+eUWhvvvf5ufYsMG01jefNO2i1ifNm0OPK01OdlmgztOKccFRIDK0ZVZuGkhm5M38/gpjzM5YTLDZw8v6mE5AK1bm0kJ7ObeurWlOgfLeDZunJmhVq0ygXD00eZEf/llc2zXrWuJCT/7zGaPA7RsaRP8RMzUtWuX9f3Xv0wolSsH559v6dPXetCCUzoJq4lJRHoCLwERwFuq+tRe27sCLwJtgctUdUTItquABwOrj6nqfjPoFYSJSVUREWSA0KFmBzbt2sT8W+cTE5VDVRLn8CI11TSPadPMaT17dlAwgGkjmZkWetunjwmGypXNpAWmsdx+O5xwgmkYSUlBTSMtzfo2bFgkl+Y4+WF/JqawCQgRiQAWAqcBCcAU4HJVnRvSpyFQEfgv8FWWgBCRKsBUoDOgwDSgk6puye18BeWDAPjg7w+IjYrlks8tXYRHNJVApk6Fyy6Dzz+HhAT49FMrqLR9u/kv/vUvEwbduplW8sorB46CatrU0qy/+qrNE6ld2/wdzz5rOa2uvNJMZW3bBnM8r19vwqpHj8KLsvIJi04IRRXFdAywWFWXBgYxHDgP2CMgVHV5YFvmXvueAfyoqpsD238EegKfhHG8gM2ivm3MbVzb4Vpu7HQjb0x7g0GTBnHXcXeF+9ROYdK5s2kSIlYbcvx4EwYZGeb4HjYs2HfAANMUYmMtM25kpKVKv/xyi6BKTDR/R3y8pRoBi6jK4n//Cy536GDHSEmxYyxZAoMHW82NadNMcN10kxUQaNIk99qZh0JyMgwdasvLllk4cGgOLcfZi3AKiDrAqpD1BODYXPrmZd86e3cSkb5AX4D69esf2ij34ojyRzDx2ok0r9acMlKGN6a9wd0/3E3l6Mpc2+HaAjmHU0wIdVYPGpR7v8qVYciQ7G0DBgSXV6+2G/sTT5iQiImBKlXg0UftppycDN27mxYxeDC8+KItX3+9jaFZM2jVKvvxW7YMzgW58ELb/uijFtrbq5ftt3Fj3utiJiebBjNqlK2ffLIdf+FCiwJznBwIp4npYqCnql4fWP8XcKyq3pZD3/eAb0JMTP8FolX1scD6Q8AuVX1u732zKEgTUxZJKUk8/8fzTF49mZ+X/UymZpLyQArlIgu4nJVTslgVeLbJLZw2NdVMTLNn2zyN336z2htgDvfFi60o8dSp8PffJoCWLDEfCZivY/lyO0atWrb93HNNOL3/Pjz1lDnxr7vOcmStXWtmL7CZ7i1b2nyRxo1tVnp6OixYYO2JiaY93XmnhQg3aGCms8suM7PUokXmt2nd2s6/aZOZyVq2LPjPMTXVggXcHBZWisrEtBoI/YfUDbTldd9ue+37S4GM6iD4YckPPDXxKcb0GYOq8tOyn2jychP+0+U/9Gzak0zNJDUjlfJly9M4vjFREf4k5nDgeRZZZqP27YPRWWBO85kz7SYM8OOPluMqM9Nuyl98YYJh1iy7uSckQLVqpoEsWWKTCOPiLE1JFg88YKHBXbuahnPCCdZ+zjlwbUAjrlzZJiqCmc/S08289eWXJjDAZq6//bYlXXz0URNcc+aYQz8y0uaeRESYv2b6dHPyh97Yt20zgfPZZ3m74a9da4WsPAtwkRJODSISc1Kfit3wpwB9VHWfAsc5aBBVMMd0x0CX6ZiTOtdSZuHQIAAWb15M0ypNUVV+XPojj//+OL+t+G2ffpFlImkc35iFmxZydfurOSLuCJ6e+DTzbp3HUVWPQg7XklJO0TF6tIXodux44LkbN91kTvYdO+x18smmUXz1Fcyfb8f46y8LAc4iMRGGD4cpU0w4RUaaOeukk+CMM+zpfeRIy5OVRd26Jpj2Jj7eZr03amRmsZUrTXikpVkgQJ8+lmLl009NuMXG2vHT0qxv8+YmeL77zvoNGWLmtGeftZn2K1ZYupS92bzZzGx738cWLzZTX9264dVASoDDv0iimAIn7oWFsUYA76jq4yIyEJiqql+JyNHAKCAeSAHWqWqrwL7XAvcHDvW4qr67v3OFS0BkMWLuCI6qehRtjmjDG1Pf4OuFX3NVu6soF1mOpJQkFmxcwPyN85myZgrbdm8jOS2ZtEyb3VsjtgYbkjdwU6ebqBFXg4G/DWTFnSuoX6lg/CaOw/r1llrkootsfcUKuwHXrGk37mOPtSf78uX3f5yUFKvvsWqV3fh27rT5JNdcA3/8YaapevXs6b5TJzMxzZljTvU1a0zobNhgWkiZMjbvJC0tqKEciM6d7Zzz5tmkxn/+sXNt3Ahjx5omEx1tfbdvt7FedFHQhAZ27hYtLIrs118P/rPMC7/9Ztrcv/8dnuMXIkUmIAqTcAqIbbu30eDFBiSlJO0T8rpo0yKOfPXIPe0yQOjVrBdfX/Y1/b7rx/jl42l7RFtmrZ/F+p3r2ZS8CcX6ZrW/1PMlqsZU5cpRV7L27rXULF8zLNfhlFJSU80EtL+Z66GMHWtmrjvuyN95hw2zglLr15uWMGeOLc+caaajlSvhySctoqpxYzN5bdli9UOGDoVnngma20LJSq1y3XUm0M45x8xoI0bY/JSRIy1IIKt2+mOPBbMJh/LVV2a+u+CCfbWAuXMtMGDhQjPhiVjocvPmNlfmvPNs3NOnm9/ouOOC2Y0PhXnzzI9zMPfjAtJe9icgUNUS8erUqZOGkwUbF2jC1oR92m/55hat8WwNTUlLUVXVnak7NTMzU1VVMzIzND0jXVVVNydv1g07NmhaRprSH+0/vr+e/O7JGjEgQulPtlft52vrWR+fpfRH//Pdf3TgLwOV/uiqrav2HNtxDgv++kt11ap923P7Hb/yimrduqobNqimpanOnq3622+qo0apDh2qev75qvHxqmeeae92iwy+IiPt/aqrVO++WzUmxtZ79rT3m29WffVVWz7ySNVmzWx5zhzVW2+15d69VaOibPmEE1QnTrQxgGqPHvb+9tuqq1erLl9u57zvvoP/bLZtCy7366d6yimqqanZ+1x7rZ1vyxZbT0xUTbF7jX75per06Qd/3r3ALDo53leL/MZeUK9wC4j8kLQrSemP9hvbT1VNiMQ8FqNP/PaE0h/9e+3f2vPDnkp/dNAfg/SKkVdom9faaO3na2vc43HZhEet52op/dH3Z76va7evLdoLc5yCJjMzeAM8EBkZdmMfPtxu4PPnqw4Zkl34bNigesUVQUGRl1d8vOptt6leeaVq+fLZt8XGqlatastly9r7WWepPvWULd9yi+pNN9ny9dcHhc7YsarJycFxjRunGhcXuAWr6ocfqvbvn/3aVFX//tuEUmam6po11v/RR1V371Zt3Fj17LPz93nr/gWEm5gKgXU71jFq3ig61OpAl7pd2L57O29Oe5OTG55M59qdSdiWQI8PevDsac9yzlHnsHjzYhZuWkjPpj0pI2VISU9h1vpZ/LX6LyYlTOKnpT+xfuf6PcdvXaM1NcvXZNzScXx0wUd0qduFxvGNsznGd6XtYlnSMmIiY2gU3yjPY8/UTMrIoafskgHiM9GdoiclxcxV69fbTPaMDDNTpaaaz2TFCns1amTmoyw/x44d5ljfuNEc/506mW/ljz8symvCBDvulpAkD2XLWgbi3btt/9RU84tER1sAwI8/2lyYb74xH8rkyWaqAjt/uXIWWHDaaSaWxo0zR3+3bmZ2q1/fruOxx+CHH6w9H3NZ3AdxmHH7mNt5Z+Y7JKclo48oo+eP5vxPz8/m5/j53z8zZc0Ulm1Zxtoda0nYlsD8jfPZmbZzz3GqxVYjPjqeRZuz23FbVm/JeUedx5MTniT1wdQcw3NVlcd+e4yHf3mYzIczDykKK1MzmbByAifVP8mjuJySzebN5thfutRCiWvUsMmXDzxgwum776B/f/PDbN+efd8TTzRhkJpq4cpjx5qQadjQ/Axz5pjjPpTQglpxcbb9EO/lLiAOM1IzUpmwcgKnNDoFgN4jejN/43z+vulvAHp80IMNOzcw6+ZZyAChfqX6nN74dN6a8RYzb5zJye+dzNbdW7m5881s3rWZnak7aVm9Je1qtmNj8kZGLxjNr8t/JUMzKF+2PCfVP4mxi8dyU6eb2JKyhU/nfMrV7a5m4qqJLNq8iFG9R9GgUgOOrHokcWXjDjh+VaXPF31Iz0xnxNwRTLx2IsfXOz6sn5njFEsSEizUFqz64e+/WzhvSoppLG3bWojw/khLs0mTv/5qWsjRR1vKluRkaxs/3sKGn3nmkIboAqKEMXXNVDbv2szpTU4HYPBfg2lYuSFnHXkWAIMmDaJ1jdac3uR0tu/eTsWnKvLEKU9w30n3IQOEEZeM4OSGJzN+2XjGL7fX4s2LqRxdmcrlKlMushxbUrawYccG0jV7KdB2R7Sja4OudG3QlUs+vyRH89H6Hes57cPTuLHTjVSOrswlrS6hbEQB5hRyHKfAcAFRitmRuoMJKyfQtEpTmlZpymdzPqP3iN6MvWIsZ358Jsv7LWfr7q20PaItAKd9eBrjlo4j5YEUykaUZUvKFhZvXsxjvz1GSnoKmZrJpIRJJKclEyERHFfvOM5ocgYPjX+IN89+E4C+3/Tlls63sG7HOr6Y/wWjLxvNifVPpEpMlaL8KBzHyQEXEM4eMjWTMYvGcFazsxAR+o3tx/t/v8/W3VvRR5SPZ31MRJkIerfqnavfYGXSSlZtW8V3i79j7OKxTFs7bZ8+8dHx1IirwbKkZaRmBKuzda7dmdoVavPVgq/47OLP6FS7E7Ur1CYtI420zDR2pu4kKSWJpJQkur7Xld0P7nbtw3HCiAsIJ1fmbJjDiq0r6NWsV576r9uxjvavt+fGTjcyoPsAFm5ayDcLv+HuH+4m4T8J3D72dkbNH7XH9JSSnsKFn17InA1z6NG4B2t2rCFhWwILNi7YM9N8f5SLKEfHWh2ZlDCJN85+g5bVW9IkvgkxUTGUiyhHuchy+YqycpzSjgsIp8BQVR797VEuaXkJLV9ryUcXfMSVo65k0e2LaFqlKe/MeIcyUoar21+9Z58dqTuIi4qjzMAy/HHtHzSv1pzYqFgmrJzAUxOeYtyycTx72rNElYkiNiqW+Jh4KkdXJikliT8T/mTy6snMWDsjW4RWKP9u92+uancV3Rp2Y1PyJmasm8EZH53BxGsncmydY4koE1FIn47jHH64gHDCxs7UnWxP3U712OpElInY77yJ9Mx0aj5Xk55Ne/LxPx+z474d1Hy+Jh+c/wEXtLiAlVtX0uDFBvx69a90bdAVGSDsuG8HcWXjyNRMVm5dydzEuaxIWkFKegq7M3azePNiPp/7Odt2b8vxnDGRMZza+FS+WfgNt3S+hZioGJ6f9DxvnP0G1WKrcdFnF2UzY2VqJut3rGfr7q3sSttFSnoKDSo3oHaF2mH7DB2nKHEB4RQLVJW/Vv9FTFTMHqd4WkbannkYM9bO4P6f7+fJU5+kfc32jF00lgs+vYDfr/mdY946hqV3LGXx5sWc1OAkoiOj2b57O0kpSVSLrcaX87+kzxd9aFi5IW+d8xYxkTE8MP4B4svFs2DzAuYmziVCIogsE8nujN3ZxhVVJoojqx7JnMQ5lI0om81nkkWXul24oPkF3DPuHoZfNJxtu7fR95u+DDlrCJFlIrnh6xuYeO1EGlRqQI24Gii6R1i6D8UpzriAcA5L5iXO4+0Zb/No90eJiYrhhUkvcNcPVvpVH1GemfgM94y7Z08Rp49mfcS8xHk80u2RfW7Kv6/4nYd/eZhPLvqEytGVGfzXYB4c/yAp6Snce8K9zEmcQ0xUDA0qNaBBpQbcNtbKzl7c4mJ6Deu1J7HioRATGUN8TDxrtq/hvKPOo0l8EwZNHsTQc4ZSLqIc6ZnpXPvVtZzf/HwWbVrEnMQ53NTpJppVbcbdP9zNH9f+Qe0KtalbsW6+zWWrt62m7gt1fXa7swcXEE6JYMPODczfOH/PzOwZa2cwfe10+rTpQ0xUzEEd68+EP/l20bcM6DYAEWHIlCHcMuaWPTfOCz69gDOanMFNnW9i0qpJnPbhaQw9ZygNKzekUnQlKparSNmIsmRkZjBv4zxO/eBUxvQZw4qtK9iYvJEyUoYyUob0zHSSUpLYsmsLicmJLN2ylKVblrIrfdc+Y2perTnNqjRjw84NLNy0kC0pW/bp075me1pVb8XH/3zMZa0vY/vu7Xy76Ft6NetFhETw9cKvue3o26gWW43+v/bnudOeQ0S4+4e7aVGtBfM2zgOgceXGDOg+gEtaXpKtQuLG5I18+PeH3PXDXVzZ9krqVaxHo8qN6PtNXxcqJRQXEI5zAM4bfh4Vy1Xkwws+3GdbwrYEBv46kOdPf56KT1XkjmPu4INZH+xJ/37DVzcwZvEYVt9lBRNlgNCmRps9M90nXzeZ2hVqU6+SVZpL3JnIztSdRJSJYG7iXN6Z8Q4fXPAB5SLLsW33NiqWqwjApuRNJGxLYM32QOTXpgX8s/4f5ibOJUMzqFCuAnFRcURFRJGRmUGGZrAzdScbkzfuI1yiI6L3zFkZvWA0a7avYcXWFQB0qNmBplWa8vncz/eY2JpXa05yWjJrtq8hPTOdiuUq0u/YftzZ5U6iI6NZvHkxy5OWc97w89jw3w1Ui61WIOlUFmxcQPPBzcl4OIOXJr/EXT/c5YIpzLiAcJwCZMyiMfy97m/uO+k+wKoOJqUk0bm2/cc++ecTVm5dyT0n3gNA2yFtKV+2PJMSJpH5cCaVnqrE9tTt+9z4Fm5aSKc3O7EjdQf6iLI8aTmNXmrEpOsm0aVuF2SAUCOuBhOvnUizV5qx+q7VRJaJpEZcjX3GmJ6ZTkp6imXlRHn+j+d5Z+Y7TOs7jRpxNcjUTMYtHcfo+aNZlrSMxZsXsz11O5e0vIQbOt5AmyPaAJCRmcHMdTN5YsITfDHvC6LKROUYnpwVddatQTeOKH8En875lAtbXEhKegpjFo3h/47/P+pVqsftY2/ni0u/IDoyml7DejHnljlUianC9LXTefnPl/l+yfcANKrciKSUJPp16ccjJz9ScF9eAZGakcpDPz/EM388c9gLsKKqSe04JZJezXplmzfStErTbNsvb3N5tvVRvUexeddmjq5j5T6HnDWEWhVq7XPcKjFVuPXoW+ndqjdghao61upIpmYCsPC2hQz8bSAZmRnoI8p1o6/jywVfsnnXZvQR5YovrmDYP8PQR5TIMpFUeLIC31/5Pac3OZ2Bvw3k/hPvp3psdQDKSBn+Wv0Xr019LVsSyDoV6uwRDkkpSQB0qt2JkZeO5J4f7+Hv9X/z/ZLvGX7RcJ6f9DxT1kzhhTNeYPHmxQyeMpgFmxawZscawLSB6MhomsQ34YXJL+wRLBd+duGea271Wqs9y7XK1+LR7o/SJL4Jr019jWVJy+j/S39e/fNVKpSrwLKkZVzT/hoqlK3Ay3+9zPdXfs9xdY+jQrkKZGomq7auYk7iHM4adhaXtrqURZsWMWPdjD0h2AXJ/I3zeXXKq7x73n4LXR72uAbhOIcpCzYuYNraafRp0weAN6a+wcbkjTzQ1aqnHff2cVQoW4Ef/vVDjvt/teArPvj7A0ZcOgKwLMILNi3gh3/9gAwQjq59NNViqzHmijHIAKHtEW05pvYxDD13KAA3fHUDHWt15OajbwbghUkv0LpGa05rchrzN87ntA9P45OLPuH4esfT6rVWVIutxlvnvMWO1B0kpSSRqZls3rWZTcmbqBpXlQubX0hURBRTVk/h6DpHM33tdMYsGsOwf4axaPMiGlduTHJ6Mtt3b2fb7m0oiiAoSmxkLMnpyXuurX7F+jSo3IDfV/4OQI/GPRi3dBxH1z6a1dtXs2b7Gno27UnDSg15fdrrPNz1YcpGlOXB8Q/y3+P+y47UHbw+7XXG9BnDcfWOo1K5SiSlJLFy60rSMtOoWb4maRlpNIpvxOSEyXSu3ZnIMofn87abmBynFLJq6ypio2KpGls1z/uEzmP5fM7nREdGc85R5wA2Kz46MjpPx5mzYQ53/3A3H1zwAUc8dwRfX/419SrWo13NdvT/pT/DZw9nwaYFZD6cSeOXG7M8aTn6iLJ2+1pqD6rN8IuG07u1aVI7U3eyfud6mrzchKV3LOWnZT9xRpMzmJc4j5u+vYlqsdU4vt7xtKjWglY1WtGociNOevckejTuwYBuA3hm4jN8MvsT4qPjqV+5PnUq1GHb7m0sS1rG8qTlbN+9PZvZLDYqlrioODbv2kyGZuwRQnsT2n5CvROYuGoiF7a4kNXbVvPn6j9pU6MNcWXjmJwwmftOvI9j6xxLh1od9gQubNu9je7vd2d63+l7vqcqMVX2fP6pGalsSt5EtdhqOabkL6haKy4gHMcpNvyy/Be+Xfgtz5z2DCLCsxOfpV3Ndpze5HQyNZOLP7uY4RcPz3H+SFa0GVioc9Y8mvTMdP416l90b9idvp368vKfL3N8vePpXLszD/z0AM9Pep7dGbvRR5SdqTtJTkumepyZ22SAkPjfRCpGVyRCIpi1fhYdanVABgiDew3mtxW/UbN8TWpXqM2IuSO4uMXFVIquxNoda1m1dRV/JPxBSnoKSSlJ1CxfkzoV6lAtthq7M3azM3UnicmJzN4wm/TM9H2uZ28iy0RSLbYa63asy9Z+7lHncnrj07lt7G3cfdzdTF0zlV9X/EqnWp04ru5xvDrl1UMWFi4gHMcpEagqizYv4siqR2ZrT0lP4aLPLuLURqdy9w93Z7tZZmomM9bOoFPtTgCcNewsxiwagz6iJKclU/O5mlzc8mLeOe8dZIBFYk24ZgIn1D+Bvl/3ZcTcEWxJ2ZLnG3BaRhplHyvLPzf/Q+sarQGr6Dh97XT+2fAPMZExVI6uTIVyFUjLSOPx3x8nNSOVPm36sG7HOpYnLadabDVqxNWgSkwV/ln/D98v+Z5lScsAy0/WrGoztu/ejoiwNWXrQY1vb/YnIIq8lnRBvYpzTWrHccJPekZ6nvoNnTZUv5j7xZ71f9b/o4s2LVJV1R27d+hrf72myalWP3p3+m6dnzj/gMf8fvH3Sn/2rB//9vF6/ejr9xxzxtoZui1lm6rqnpryG3ZsUPqjL0x6Qd+Z/o5mZmbqmm1rNHJg5J5jbU3ZqvRHJ6+arLPXz9amLzXVq7+8ep/zZ2Rm5Onac4L91KQ+PL0qjuM4e5HXWebXd7w+23rWUz5AXNm4PU53gLIRZTmq2lEHPOaSzUuoElOF3em7KRdZjq8u+4oMtZKg5Z8sD8D4q8bTrWE3vrviO3p+3JOJ107c56k/OjKaZ3o8Q9cGXQELoa5fqT6ZmknL6i1ZvGUxb5371j7nD1dGYzcxOY7jFACqmuNkwU3Jm/h1xa+cVP8kqsdVJzktmXU71lG7Qu08Of0zNRNBwlbXfX8mprAm0heRniKyQEQWi8i9OWwvJyKfBrb/KSINA+0NRWSXiMwMvF4P5zgdx3HyS2438KqxVbmwxYV7nOKxUbE0jm+c54iwMlImbMLhQITNxCQiEcBg4DQgAZgiIl+p6tyQbtcBW1S1qYhcBjwN9A5sW6Kq7cM1PsdxHGf/hFODOAZYrKpLVTUVGA6ct1ef84D3A8sjgFOlqESl4ziOk41wCog6wKqQ9YRAW459VDUd2ApkzeppJCIzRORXETkppxOISF8RmSoiUxMTEwt29I7jOKWc4lrMdy1QX1U7AHcBw0Sk4t6dVPVNVe2sqp2rV69e6IN0HMcpyYRTQKwG6oWs1w205dhHRCKBSsAmVd2tqpsAVHUasAQ4EsdxHKfQCKeAmAI0E5FGIlIWuAz4aq8+XwFXBZYvBn5WVRWR6gEnNyLSGGgGLA3jWB3HcZy9CFsUk6qmi8htwPdABPCOqs4RkYHYzL2vgLeBD0VkMbAZEyIAXYGBIpIGZAI3qermcI3VcRzH2RefKOc4jlOKKRXJ+kQkEViRj0NUAzYW0HAOF0rjNUPpvO7SeM1QOq/7YK+5garmGOVTYgREfhGRqblJ0ZJKabxmKJ3XXRqvGUrndRfkNRfXMFfHcRyniHEB4TiO4+SIC4ggbxb1AIqA0njNUDqvuzReM5TO6y6wa3YfhOM4jpMjrkE4juM4OeICwnEcx8mRUi8gDlTUqKQgIvVEZLyIzBWROSLSL9BeRUR+FJFFgff4oh5rQSMiEYHMwN8E1hsFClQtDhSsKlvUYyxoRKSyiIwQkfkiMk9Ejivp37WI/Cfw254tIp+ISHRJ/K5F5B0R2SAis0PacvxuxXg5cP2zRKTjwZyrVAuIkKJGZwItgctFpGXRjipspAN3q2pLoAtwa+Ba7wV+UtVmwE+B9ZJGP2BeyPrTwAuq2hTYghWuKmm8BHynqs2Bdtj1l9jvWkTqAHcAnVW1NZbeJ6sIWUn7rt8Deu7Vltt3eyaWy64Z0BcYcjAnKtUCgrwVNSoRqOpaVZ0eWN6O3TDqkL1o0/vA+UUywDAhInWBs4C3AusCnIIVqIKSec2VsHxmbwOoaqqqJlHCv2sst1xMIDN0LFY2oMR916r6G5a7LpTcvtvzgA/UmAxUFpFaeT1XaRcQeSlqVOII1P7uAPwJHKGqawOb1gFHFNW4wsSLwP9hSR/BClIlBQpUQcn8zhsBicC7AdPaWyISRwn+rlV1NfAcsBITDFuBaZT87zqL3L7bfN3jSruAKHWISHlgJHCnqm4L3aYW81xi4p5F5GxgQ6CmSGkiEugIDAkU3drJXuakEvhdx2NPy42A2kAc+5phSgUF+d2WdgGRl6JGJQYRicKEw8eq+kWgeX2Wyhl431BU4wsDJwDnishyzHx4CmabrxwwQ0DJ/M4TgARV/TOwPgITGCX5u+4BLFPVRFVNA77Avv+S/l1nkdt3m697XGkXEHkpalQiCNje3wbmqeqgkE2hRZuuAkYX9tjCharep6p1VbUh9t3+rKpXAOOxAlVQwq4ZQFXXAatE5KhA06nAXErwd42ZlrqISGzgt551zSX6uw4ht+/2K+DfgWimLsDWEFPUASn1M6lFpBdmp84qavR40Y4oPIjIicDvwD8E7fH3Y36Iz4D6WLr0S0ticSYR6Qb8V1XPDlQpHA5UAWYAV6rq7iIcXoEjIu0xx3xZrBrjNdgDYYn9rkVkANAbi9ibAVyP2dtL1HctIp8A3bC03uuBR4AvyeG7DQjLVzFzWzJwjarmuXBOqRcQjuM4Ts6UdhOT4ziOkwsuIBzHcZwccQHhOI7j5IgLCMdxHCdHXEA4juM4OeICwnEOAhHJEJGZIa8CS3gnIg1DM3Q6TlETeeAujuOEsEtV2xf1IBynMHANwnEKABFZLiLPiMg/IvKXiDQNtDcUkZ8Dufh/EpH6gfYjRGSUiPwdeB0fOFSEiAwN1DX4QURiiuyinFKPCwjHOThi9jIx9Q7ZtlVV22AzV18MtL0CvK+qbYGPgZcD7S8Dv6pqOyxP0pxAezNgsKq2ApKAi8J6NY6zH3wmteMcBCKyQ1XL59C+HDhFVZcGkiKuU9WqIrIRqKWqaYH2tapaTUQSgbqhaR8Cadh/DBR9QUTuAaJU9bFCuDTH2QfXIByn4NBclg+G0DxBGbif0ClCXEA4TsHRO+R9UmD5DyyTLMAVWMJEsLKQN8OemtmVCmuQjpNX/OnEcQ6OGBGZGbL+napmhbrGi8gsTAu4PNB2O1bZ7X9YlbdrAu39gDdF5DpMU7gZq4TmOMUG90E4TgEQ8EF0VtWNRT0Wxyko3MTkOI7j5IhrEI7jOE6OuAbhOI7j5IgLCMdxHCdHXEA4juM4OeICwnEcx8kRFxCO4zhOjvw//YLWQf/c8akAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [6.64536357e-02 8.40425491e-06 1.96695328e-06 ... 1.76950991e-02\n",
      "  3.89152169e-02 5.66244125e-07]\n",
      " [2.18766630e-02 1.19537115e-04 4.32133675e-06 ... 2.17006147e-01\n",
      "  6.71148300e-05 5.66244125e-07]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [4.32133675e-06 2.00569630e-05 2.04741955e-05 ... 5.02904058e-02\n",
      "  9.38445330e-04 1.32322311e-05]\n",
      " [7.67573714e-03 1.63248628e-01 2.81972289e-02 ... 1.63160563e-02\n",
      "  6.97925389e-02 4.57677245e-02]]\n",
      "……………………………………………………………………………………………\n",
      "开始进行模型评估\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "0.5429746698419571\n",
      "|||||||||||||||||||||||||||||||\n",
      "缺陷率\n",
      "accuracy\n",
      "recall-avg\n",
      "recall-0-defect\n",
      "recall-1-nodefect\n",
      "precision-avg\n",
      "precision-0-defect\n",
      "precision-1-nodefect\n",
      "F1-score\n",
      "F1-score-0-defect\n",
      "F1-score-1-nodefect\n",
      "|||||||||||||||||||||||||||||||\n",
      "0.5429746698419571\n",
      "0.9333189001948474\n",
      "0.9271612696311642\n",
      "0.8555187115111321\n",
      "0.9988038277511961\n",
      "0.9449003499701965\n",
      "0.9983416252072969\n",
      "0.891459074733096\n",
      "0.9317560307312094\n",
      "0.9214285714285714\n",
      "0.9420834900338473\n",
      "|||||||||||||||||||||||||||||||\n",
      "=====================================================================\n",
      "……………………………………………………………………………………………\n",
      "……………………………………………………………………………………………\n",
      "……………………………………………………………………………………………\n",
      "开始进行任务2评估\n",
      "############### jts_java ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Programming\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "F:\\Programming\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1的sub-accuracy\n",
      "0.9639261744966443\n",
      "##############################\n",
      "Top1的sub-precision\n",
      "Task2的Precision的分子 2194.0\n",
      "Task2的Precision的分母 2686\n",
      "0.8168279970215935\n",
      "##############################\n",
      "Top1的sub-recall\n",
      "Task2的Recall的分子 2153.0\n",
      "Task2的Recall的分母 2485\n",
      "0.8663983903420523\n",
      "##############################\n",
      "data length:  4619\n",
      "top 1:\n",
      "sum =  3673\n",
      "rate =  0.795193764884174\n",
      "##############################\n",
      "top 3:\n",
      "sum =  4210\n",
      "rate =  0.9114526953886123\n",
      "##############################\n",
      "top 5:\n",
      "sum =  4520\n",
      "rate =  0.9785667893483438\n",
      "##############################\n",
      "top 10:\n",
      "sum =  4570\n",
      "rate =  0.9893916432128166\n",
      "##############################\n",
      "top10范围下的平均排名MRR：\n",
      "1.4277899343544858\n",
      "##############################\n",
      "各类别缺陷分布统计:\n",
      "[44, 28, 35, 2107, 1430, 84, 78, 28]\n",
      "##############################\n",
      "各类别缺陷预测正确率统计:\n",
      "[14, 0, 7, 2010, 1218, 25, 31, 6]\n",
      "|||||||||||||||||||||||||||||||\n",
      "0.9639261744966443\n",
      "0.8168279970215935\n",
      "0.8663983903420523\n",
      "0.795193764884174\n",
      "0.9114526953886123\n",
      "0.9785667893483438\n",
      "0.9893916432128166\n",
      "0.00952587140073609\n",
      "0.006061918164104785\n",
      "0.007577397705130981\n",
      "0.456159341848885\n",
      "0.30959082052392295\n",
      "0.018185754492314354\n",
      "0.016886772028577614\n",
      "0.006061918164104785\n",
      "0.3181818181818182\n",
      "0.0\n",
      "0.2\n",
      "0.9539629805410537\n",
      "0.8517482517482518\n",
      "0.2976190476190476\n",
      "0.3974358974358974\n",
      "0.21428571428571427\n",
      "1.4277899343544858\n",
      "=====================================================================\n",
      "……………………………………………………………………………………………\n",
      "……………………………………………………………………………………………\n",
      "……………………………………………………………………………………………\n",
      "开始进行任务3评估\n",
      "############### jts_java ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Programming\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:270: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "F:\\Programming\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1的sub-accuracy\n",
      "0.9736631305477239\n",
      "##############################\n",
      "Top1的sub-precision\n",
      "Task3的Precision的分子 1540.3333333333335\n",
      "Task3的Precision的分母 2173\n",
      "0.7088510507746587\n",
      "##############################\n",
      "Top1的sub-recall\n",
      "Task3的Recall的分子 1438.0\n",
      "Task3的Recall的分母 2508\n",
      "0.5733652312599681\n",
      "##############################\n",
      "data length:  4619\n",
      "top 1:\n",
      "sum =  3013\n",
      "rate =  0.6523056938731328\n",
      "##############################\n",
      "top 3:\n",
      "sum =  3728\n",
      "rate =  0.8071011041350942\n",
      "##############################\n",
      "top 5:\n",
      "sum =  4095\n",
      "rate =  0.8865555315003247\n",
      "##############################\n",
      "top 10:\n",
      "sum =  4314\n",
      "rate =  0.9339683914267157\n",
      "##############################\n",
      "top10范围下的平均排名MRR：\n",
      "1.8291608715808994\n",
      "##############################\n",
      "各类别缺陷分布统计:\n",
      "[0, 68, 347, 429, 477, 648, 476, 418, 366, 160, 72, 58, 59, 31, 23, 14, 0, 0, 0, 0]\n",
      "##############################\n",
      "各类别缺陷预测正确率统计:\n",
      "[0, 54, 253, 234, 266, 355, 296, 261, 195, 70, 24, 26, 19, 0, 0, 5, 0, 0, 0, 0]\n",
      "|||||||||||||||||||||||||||||||\n",
      "0.9736631305477239\n",
      "0.7088510507746587\n",
      "0.5733652312599681\n",
      "0.6523056938731328\n",
      "0.8071011041350942\n",
      "0.8865555315003247\n",
      "0.9339683914267157\n",
      "0.0\n",
      "0.014721801255683049\n",
      "0.07512448581944144\n",
      "0.09287724615717688\n",
      "0.10326910586707079\n",
      "0.14029010608356787\n",
      "0.10305260878978134\n",
      "0.09049577830699286\n",
      "0.07923793028794111\n",
      "0.03463953236631306\n",
      "0.015587789564840875\n",
      "0.012556830482788483\n",
      "0.012773327560077938\n",
      "0.006711409395973154\n",
      "0.0049794327776575015\n",
      "0.0030309590820523924\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0\n",
      "0.7941176470588235\n",
      "0.729106628242075\n",
      "0.5454545454545454\n",
      "0.5576519916142557\n",
      "0.5478395061728395\n",
      "0.6218487394957983\n",
      "0.6244019138755981\n",
      "0.5327868852459017\n",
      "0.4375\n",
      "0.3333333333333333\n",
      "0.4482758620689655\n",
      "0.3220338983050847\n",
      "0.0\n",
      "0.0\n",
      "0.35714285714285715\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1.8291608715808994\n",
      "=====================================================================\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "show_train_history(train_history,'output1_acc','output2_acc','output3_acc','val_output1_acc','val_output2_acc','val_output3_acc')\n",
    "show_train_history(train_history,'output1_loss','output2_loss','output3_loss','val_output1_loss','val_output2_loss','val_output3_loss')    \n",
    "\n",
    "#模型预测\n",
    "predProb = model.predict(test_Features1,batch_size = 30)\n",
    "\n",
    "#模型评分\n",
    "#     score = average_precision_score(test_Label1,predProb[0])\n",
    "#     print(\"******* score *********\")\n",
    "#     print(score)\n",
    "#     print(\"***********************\")\n",
    "# 保存整个模型\n",
    "#     model.save('kfold/model_' + software + str(k) +'.h5')\n",
    "#     print(\"Model saved!\")\n",
    "print(predProb[1])\n",
    "#模型评估\n",
    "#任务1评估\n",
    "eval1(predProb[0],test_Label1)\n",
    "#任务2评估\n",
    "eval2(predProb[1],test_df2)\n",
    "#任务3评估\n",
    "eval3(predProb[2],test_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- activations -----\n",
      "(4619, 1, 96)\n",
      "(4619, 96)\n"
     ]
    }
   ],
   "source": [
    "attention_vector1 = np.mean(get_activations(model,\n",
    "                                           test_Features1,\n",
    "                                           print_shape_only=True,\n",
    "                                           layer_name='attention_vec1')[0], axis=1).squeeze()\n",
    "print(attention_vector1.shape)\n",
    "# attention_vector2 = np.mean(get_activations(model,\n",
    "#                                            test_Features1[0:50],\n",
    "#                                            print_shape_only=True,\n",
    "#                                            layer_name='attention_vec2')[0], axis=1).squeeze()\n",
    "# print(attention_vector2.shape)\n",
    "# attention_vector3 = np.mean(get_activations(model,\n",
    "#                                            test_Features1[0:50],\n",
    "#                                            print_shape_only=True,\n",
    "#                                            layer_name='attention_vec3')[0], axis=1).squeeze()\n",
    "# print(attention_vector3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.187469090001398, 0.1435903078128067, 0.14253662639317086, 0.14243164258985042, 0.14136675931737336, 0.1410741446673926, 0.14029848040022444, 0.13959782777500235, 0.13779147691973928, 0.13709134438944964, 0.13650513505387654, 0.1362772628876052, 0.13446784866783337, 0.13319309937568427, 0.13156485109904004, 0.12970652836884597, 0.12783534731852822, 0.12639111105203787, 0.12559819443032622, 0.127330895564573, 0.12497448274019678, 0.12281670503106233, 0.12215896944868744, 0.12157283628566785, 0.12242115888066045, 0.11895941429066177, 0.11757995609801895, 0.11627168627636296, 0.11631993865080255, 0.11504414552364951, 0.11313920247750726, 0.11224546578997646, 0.11209286947301662, 0.11223324652914056, 0.1118250920697519, 0.11168181103558746, 0.11046511812211994, 0.10766147049005761, 0.10949458352415921, 0.10643614927755071, 0.1054718040510341, 0.10483515485832434, 0.10368737286064024, 0.10552762398506475, 0.10421034797267387, 0.10225416697909229, 0.10252651204726569, 0.10262782713810295, 0.10132000382656595, 0.1008464069271588, 0.1014708801217852, 0.10032594763414596, 0.09884120080658383, 0.09996802118071321, 0.09924490499027301, 0.09846501354166455, 0.09744798017387704, 0.09747439611528418, 0.097165159746202, 0.09677197532281509, 0.09722347452501777, 0.09975112663398018, 0.09645705274539597, 0.09563530211790064, 0.09737037825788325, 0.09680090414204094, 0.09431433721781414, 0.09534726053930358, 0.09457011309283216, 0.09417028073881074, 0.09425876235740478, 0.09348122340597269, 0.0932526319589778, 0.09334090553183265, 0.09464396488060373, 0.09225970889701414, 0.09193264190110073, 0.09149917148399142, 0.09041261132425697, 0.09104927862517323, 0.09185071286452767, 0.08936771827216106, 0.08933114569589892, 0.08924941147861674, 0.08868302876229563, 0.09048508910369414, 0.08861465485584571, 0.08979859180398983, 0.08834750777787262, 0.08768306695971279, 0.08815230147697288, 0.08803014965514105, 0.08671607085485343, 0.08600840856051455, 0.08734699717725197, 0.08672929399654411, 0.08679041158610545, 0.08560641407847133, 0.0865763076417398, 0.08539327353410314]\n",
      "[0.1581993452257752, 0.11040067227068677, 0.1021614429204521, 0.09717973279360381, 0.09495698009047233, 0.0950777444983013, 0.09437414256428456, 0.09394473971015674, 0.09278219062389417, 0.09237711673912861, 0.09099019671484507, 0.09063843173026784, 0.08971003136735349, 0.08943877056426229, 0.0884605407680991, 0.08716173114965231, 0.08672787024366112, 0.08571351356291602, 0.08539752908808718, 0.08603288424149659, 0.08459150592034695, 0.08340595200902778, 0.08372078755812962, 0.08334889671038805, 0.08270754551231965, 0.08159465296063276, 0.08024887637660001, 0.08020727316294222, 0.08068615822219823, 0.07957271808619233, 0.0790844165962615, 0.0781416911534745, 0.07823276419412098, 0.07852972111475909, 0.07751662653428928, 0.07674333175685848, 0.07661574793805034, 0.0748778011012122, 0.0757674138246772, 0.07454504197921151, 0.07439124481046835, 0.07402775676595676, 0.07320763480371768, 0.07377824537255906, 0.07392858494021876, 0.07244715116769733, 0.07270128689741173, 0.07263497503845988, 0.07207781639531081, 0.07211866403516563, 0.07180374037246884, 0.0711694613893641, 0.0710647232421305, 0.07184902874902976, 0.07116051092066819, 0.07068287402731951, 0.06989555662943689, 0.07012520199157442, 0.07007489868427412, 0.06971937774433007, 0.0697026860492124, 0.07094043256301986, 0.06898502304853323, 0.06833401082359133, 0.07016851525386833, 0.06964746154331992, 0.06735434806113509, 0.06802973621759474, 0.06736016951946433, 0.06695690887780906, 0.0670914317251096, 0.06655727529687454, 0.0664111942562623, 0.06608255531479056, 0.06808514769044563, 0.06608803104497238, 0.06551738367824811, 0.06527696067192777, 0.06415839949838063, 0.06492441714498969, 0.06578447031720698, 0.0638565583238942, 0.06438168907120125, 0.06379415674928388, 0.06369191389339231, 0.06390622235584686, 0.0628382873099253, 0.06334096292681224, 0.06288283816214536, 0.06228785451885438, 0.06277572733605113, 0.06258040313018638, 0.061577312622409, 0.061860830385796005, 0.06169140228577736, 0.06127082516559859, 0.06165978881068349, 0.06038486760407866, 0.061114692756589775, 0.06075629475996342]\n"
     ]
    }
   ],
   "source": [
    "print(train_history.history['output2_loss'])\n",
    "print(train_history.history['output3_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9255211805892662, 0.9323967656444997, 0.9326573577169969, 0.9321562208451324, 0.9323867432067184, 0.9323199245758622, 0.9330415607661658, 0.9337331277394094, 0.9359180810106449, 0.9366898296527105, 0.9366397173044979, 0.9366163306180191, 0.9373613535425934, 0.9375918748686853, 0.9382132828020562, 0.9396298277765198, 0.9404149419019109, 0.940892691744787, 0.9413704409743318, 0.940939464265453, 0.9413002814565387, 0.94263998443042, 0.9432613932001516, 0.9436155293979668, 0.9436121880092134, 0.9456000274218261, 0.9464653213919763, 0.9470098909330642, 0.9465555265101777, 0.9478751839712695, 0.948630229755788, 0.9489643192699085, 0.9490511818682783, 0.9491079772329579, 0.949405318639598, 0.9493552049771041, 0.9502071360367338, 0.9516203393436571, 0.9507750895640501, 0.9524188163477072, 0.9528464517315552, 0.9533843376747064, 0.9539021778103504, 0.9534812230454016, 0.9537050641189997, 0.9544166777201807, 0.9545068811656606, 0.9545503131737608, 0.9550080170976148, 0.9552184945995693, 0.9548242673682096, 0.9556862212877836, 0.9563711073314795, 0.9557597208322568, 0.9560403588874987, 0.956825470025887, 0.9571461972906105, 0.9572196963093712, 0.9575404240759111, 0.957807696377006, 0.9576573555169697, 0.9565882649505169, 0.9580181739904752, 0.9579346495787918, 0.9572564485030713, 0.9574869698132326, 0.9585259929229658, 0.9578644920523339, 0.9585193088630392, 0.958546034827456, 0.9585393550210214, 0.9589001730166063, 0.958997059120113, 0.958672990602047, 0.958706399423624, 0.9598155815334032, 0.9595917402447409, 0.9600394214759229, 0.9604403301067854, 0.9602131486241708, 0.9601363081608993, 0.9606842162311834, 0.9609347825762137, 0.9606641710210767, 0.9611419197169437, 0.9603100346320934, 0.9609949209386455, 0.9608579432280898, 0.961937055228691, 0.9618902842692317, 0.9615394877715177, 0.9612187608731999, 0.9622878508183562, 0.9623880781831707, 0.961963783487126, 0.9620239195714704, 0.9622544424906303, 0.9630161679222767, 0.9623212602293684, 0.9628858729414358]\n",
      "[0.9588133010908226, 0.9600387447007468, 0.9604516787578695, 0.9607202821789173, 0.9609167317121914, 0.9608111609339507, 0.9609407864836597, 0.9611559357376129, 0.9614953726796914, 0.9618308016455112, 0.9619791377567279, 0.9619965090497394, 0.9619337006891601, 0.9621354952368939, 0.9621862754311359, 0.962583173712068, 0.9625724844318906, 0.9627996657631637, 0.9628811830762521, 0.9627836287682761, 0.9632273001513443, 0.9634464667328663, 0.963370290586978, 0.9633248588322924, 0.9634972456685843, 0.9637885741191514, 0.9643979551447086, 0.9645476291065912, 0.9642736734624503, 0.9646117751425988, 0.9647240311437059, 0.9650220378952582, 0.9649057737585855, 0.964875038368218, 0.9650995455673317, 0.9652652589853922, 0.9653962205218771, 0.9661325582300742, 0.9656167224131212, 0.9661886855416257, 0.9662234302703254, 0.966385132247204, 0.9666965016225244, 0.9663891401517747, 0.9663383554571152, 0.9671856136416331, 0.9669771414381065, 0.9672337228579906, 0.9673352847678544, 0.9673580026683938, 0.967551777496004, 0.9678725054298161, 0.9676413133662763, 0.9675277223342341, 0.9676586866028309, 0.9681558143354969, 0.9682760871095516, 0.9683228571291007, 0.9683656228922052, 0.9686970380989288, 0.9686689753750979, 0.9679754062815973, 0.9685634048597134, 0.9690525128483597, 0.9682867775845302, 0.9683910143912261, 0.9693892731491964, 0.968906847404379, 0.9695175683356599, 0.9693839305323043, 0.9694961862147977, 0.9697327244637023, 0.9698436404555363, 0.9698570028323789, 0.9692142115753792, 0.969874373440371, 0.9703260657702253, 0.9705505755979015, 0.9708205214417205, 0.9703461119680342, 0.9701723824381494, 0.9708352200442993, 0.9706267503339244, 0.9708672945757179, 0.9712120756401379, 0.9710410234763408, 0.9715167676304872, 0.9712601838050704, 0.9714419280687756, 0.9718695663918344, 0.9716891594291864, 0.9717853780371388, 0.9720018656674704, 0.9719003020131969, 0.9722691381278721, 0.9721956377868647, 0.9721221376609216, 0.9726673750161774, 0.9723680281358719, 0.9726045668626969]\n"
     ]
    }
   ],
   "source": [
    "print(train_history.history['output2_acc'])\n",
    "print(train_history.history['output3_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6760065e-07 4.4304891e-03 5.2483682e-07 2.6656135e-03 2.9256074e-07\n",
      " 3.4249281e-03 1.5997453e-02 2.1991711e-07 3.0993249e-07 3.2975851e-07\n",
      " 3.2941131e-07 3.1830490e-07 8.5117761e-05 2.9295938e-07 1.3719314e-04\n",
      " 1.0389703e-06 3.0818001e-07 3.0191285e-07 1.9133297e-07 5.9230113e-04\n",
      " 5.4042782e-03 3.7092815e-07 2.0379784e-04 1.0477768e-03 4.5271283e-03\n",
      " 2.5604115e-07 2.8482009e-07 7.6751235e-07 1.0317505e-02 3.2530119e-07\n",
      " 2.7801170e-07 1.4672695e-02 2.8802825e-07 4.5877758e-02 3.9453948e-07\n",
      " 7.2154313e-07 2.6746152e-03 3.1893283e-02 4.0411570e-07 2.7097582e-07\n",
      " 8.9926030e-03 1.1700257e-02 1.6137339e-02 5.6894687e-06 3.3024038e-04\n",
      " 1.7347154e-05 2.5746917e-07 3.1615585e-07 1.6424718e-03 6.8950567e-07\n",
      " 5.8893594e-03 2.4698667e-07 1.2153827e-06 4.9838422e-07 4.1594248e-07\n",
      " 3.1048802e-07 5.2456835e-06 7.7639767e-03 8.9577370e-05 3.3480984e-07\n",
      " 2.9592286e-07 4.0363528e-07 1.8194217e-02 9.8627759e-03 2.7041884e-07\n",
      " 3.5933257e-07 5.1769808e-07 3.4245500e-07 5.7304036e-02 2.6810179e-07\n",
      " 4.2803700e-07 8.4342418e-04 6.5051852e-04 2.5772649e-01 3.4050474e-01\n",
      " 1.4890759e-06 5.0092035e-07 3.3358572e-07 4.3701166e-07 1.9957895e-06\n",
      " 1.4559218e-06 1.0974959e-06 3.2619312e-07 3.2248815e-07 3.7263069e-07\n",
      " 4.1598685e-07 2.5937706e-07 1.2939266e-06 1.3406052e-02 1.6152779e-02\n",
      " 3.4696933e-07 2.0971761e-06 1.5664298e-06 8.8790946e-02 5.2855571e-06\n",
      " 2.7794732e-07]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Attention'}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_vector_final1 = np.mean(np.array(attention_vector1), axis=0)\n",
    "print(attention_vector_final1)\n",
    "pd.DataFrame(attention_vector1[50], columns=['attention (%)']).plot(kind='bar',\n",
    "                                                                    title='Attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
